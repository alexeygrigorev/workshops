{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "383ff68e-bcca-4d92-8ddb-ae65e7a558d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69326914-83db-4b30-a783-0920c42de277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "848bb9af-8949-4f28-8f59-c040b2431da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x2306c6a4050>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import AppendableIndex\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bccd086-9cee-4202-8ebc-cf830bccc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b3dec-9e2a-43ff-a84b-e7c78c18439a",
   "metadata": {},
   "source": [
    "RAG - retrieval augmented generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9f155f2-4f60-4609-83b9-c17c8db2d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "If you want to look up the answer, explain why before making the call. Use as many \n",
    "keywords from the user question as possible when making first requests.\n",
    "\n",
    "Make multiple searches. Try to expand your search by using new keywords based on the results you\n",
    "get from the search.\n",
    "\n",
    "At the end, make a clarifying question based on what you presented and ask if there are \n",
    "other areas that the user wants to explore.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "942c362c-aaaf-4665-9384-df13ba3da16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.llm import OpenAIChatCompletionsClient\n",
    "\n",
    "from toyaikit.tools import Tools\n",
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import DisplayingRunnerCallback\n",
    "from toyaikit.chat.runners import OpenAIChatCompletionsRunner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "496694ac-1f78-45a3-9cb6-efdffe78a1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "groq_client = OpenAI(\n",
    "    api_key=os.getenv('GROQ_API_KEY'),\n",
    "    base_url='https://api.groq.com/openai/v1'\n",
    ")\n",
    "\n",
    "llm_client = OpenAIChatCompletionsClient(\n",
    "    model='openai/gpt-oss-20b',\n",
    "    client=groq_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79da53e-9989-4d4d-882f-70af122d4185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "941be8a3-41a8-4a89-97bc-337dbbbdfb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_tools = Tools()\n",
    "agent_tools.add_tool(search, search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25d21bde-b754-4398-9190-59f87718a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_interface = IPythonChatInterface()\n",
    "\n",
    "runner = OpenAIChatCompletionsRunner(\n",
    "    tools=agent_tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=llm_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "417760f9-0930-43b9-8633-004bee1720ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = DisplayingRunnerCallback(chat_interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b508aea8-7c54-4983-a56f-048e46bb19a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"install Kafka\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"install Kafka\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"In my set up, all of the dependencies listed in gradle.build were not installed in &lt;project_name&gt;-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \\\"java-kafka-rides\\\"\\narchiveClassifier = ''\\n}\\nAnd then in the command line ran \\u2018gradle shadowjar\\u2019, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Java Kafka: &lt;project_name&gt;-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Kafka- python videos have low audio and hard to follow up\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"kafka.errors.NoBrokersAvailable: NoBrokersAvailable\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \\\"main\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"install kafka course FAQ\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"install kafka course FAQ\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"In my set up, all of the dependencies listed in gradle.build were not installed in &lt;project_name&gt;-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \\\"java-kafka-rides\\\"\\narchiveClassifier = ''\\n}\\nAnd then in the command line ran \\u2018gradle shadowjar\\u2019, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Java Kafka: &lt;project_name&gt;-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Kafka- python videos have low audio and hard to follow up\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"kafka.errors.NoBrokersAvailable: NoBrokersAvailable\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \\\"main\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"install kafka docker\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"install kafka docker\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"kafka.errors.NoBrokersAvailable: NoBrokersAvailable\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"In my set up, all of the dependencies listed in gradle.build were not installed in &lt;project_name&gt;-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \\\"java-kafka-rides\\\"\\narchiveClassifier = ''\\n}\\nAnd then in the command line ran \\u2018gradle shadowjar\\u2019, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Java Kafka: &lt;project_name&gt;-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Kafka- python videos have low audio and hard to follow up\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \\\"main\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><h3>How to Install Kafka</h3>\n",
       "<p>Below is a quick‑start guide that works on most operating systems (Linux, macOS, Windows).<br />\n",
       "Feel free to pick the route that best matches your environment—plain binaries, Docker, or the Confluent Platform.</p>\n",
       "<hr />\n",
       "<h2>1.  Decide How You Want to Run Kafka</h2>\n",
       "<table>\n",
       "<thead>\n",
       "<tr>\n",
       "  <th>Option</th>\n",
       "  <th>When to use it</th>\n",
       "  <th>What you’ll install</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "  <td><strong>Plain Java binaries</strong></td>\n",
       "  <td>You want full control, no Docker.</td>\n",
       "  <td>OpenJDK + Apache Kafka tarball</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td><strong>Docker / Docker‑Compose</strong></td>\n",
       "  <td>You prefer isolated containers or a quick spin‑up.</td>\n",
       "  <td>Official Kafka + Zookeeper images</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td><strong>Confluent Platform</strong></td>\n",
       "  <td>You need schema registry, REST proxy, or Confluent‑specific tools.</td>\n",
       "  <td><code>confluent</code> CLI (includes Kafka, Zookeeper, etc.)</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<p>Below I’ll show the two most common approaches: <strong>plain binaries</strong> and <strong>Docker</strong>.</p>\n",
       "<hr />\n",
       "<h2>2.  Install Using Plain Java Binaries</h2>\n",
       "<blockquote>\n",
       "<p><strong>Prerequisite</strong> – Java ≥ 11 (OpenJDK works fine).</p>\n",
       "<pre><code class=\"language-bash\"># macOS (Homebrew)\n",
       "brew install openjdk@17\n",
       "echo 'export PATH=&quot;/usr/local/opt/openjdk@17/bin:$PATH&quot;' &gt;&gt; ~/.zshrc\n",
       "source ~/.zshrc\n",
       "\n",
       "# Ubuntu / Debian\n",
       "sudo apt update &amp;&amp; sudo apt install openjdk-17-jdk\n",
       "\n",
       "# Windows\n",
       "# Download and install the latest OpenJDK from https://openjdk.java.net/install/\n",
       "</code></pre>\n",
       "</blockquote>\n",
       "<ol>\n",
       "<li><p><strong>Download Kafka</strong></p>\n",
       "<pre><code class=\"language-bash\">wget https://downloads.apache.org/kafka/3.6.0/kafka_2.13-3.6.0.tgz\n",
       "tar -xzf kafka_2.13-3.6.0.tgz\n",
       "cd kafka_2.13-3.6.0\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Start Zookeeper (Kafka’s coordination service)</strong></p>\n",
       "<pre><code class=\"language-bash\">bin/zookeeper-server-start.sh config/zookeeper.properties\n",
       "# or on Windows: bin\\windows\\zookeeper-server-start.bat config\\zookeeper.properties\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Start the Kafka broker</strong></p>\n",
       "<p>Open a new terminal (or new tab) and run:</p>\n",
       "<pre><code class=\"language-bash\">bin/kafka-server-start.sh config/server.properties\n",
       "# or on Windows: bin\\windows\\kafka-server-start.bat config\\server.properties\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Verify the installation</strong></p>\n",
       "<pre><code class=\"language-bash\"># Create a topic\n",
       "bin/kafka-topics.sh --create --topic my‑topic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1\n",
       "\n",
       "# List topics\n",
       "bin/kafka-topics.sh --list --bootstrap-server localhost:9092\n",
       "\n",
       "# Produce a message\n",
       "bin/kafka-console-producer.sh --topic my‑topic --bootstrap-server localhost:9092\n",
       "&gt; Hello, Kafka!\n",
       "\n",
       "# Consume the message\n",
       "bin/kafka-console-consumer.sh --topic my‑topic --from-beginning --bootstrap-server localhost:9092\n",
       "</code></pre>\n",
       "<p>You should see the <em>Hello, Kafka!</em> message in the consumer.</p>\n",
       "</li>\n",
       "<li><p><strong>(Optional) Configure your environment</strong></p>\n",
       "<ul>\n",
       "<li>Set <code>KAFKA_HOME</code> to the Kafka root directory.</li>\n",
       "<li>Add <code>$KAFKA_HOME/bin</code> to your PATH for easier command usage.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ol>\n",
       "<hr />\n",
       "<h2>3.  Install Using Docker / Docker‑Compose</h2>\n",
       "<blockquote>\n",
       "<p><strong>Prerequisite</strong> – Docker Engine and Docker‑Compose installed.<br />\n",
       "On most systems you can get them from https://docs.docker.com/get-docker/</p>\n",
       "</blockquote>\n",
       "<ol>\n",
       "<li><p><strong>Create a <code>docker‑compose.yml</code></strong></p>\n",
       "<pre><code class=\"language-yaml\">version: &quot;3.8&quot;\n",
       "\n",
       "services:\n",
       "  zookeeper:\n",
       "    image: confluentinc/cp-zookeeper:7.5.0\n",
       "    container_name: zookeeper\n",
       "    environment:\n",
       "      ZOOKEEPER_CLIENT_PORT: 2181\n",
       "      ZOOKEEPER_TICK_TIME: 2000\n",
       "\n",
       "  kafka:\n",
       "    image: confluentinc/cp-kafka:7.5.0\n",
       "    container_name: kafka\n",
       "    depends_on:\n",
       "      - zookeeper\n",
       "    ports:\n",
       "      - &quot;9092:9092&quot;\n",
       "    environment:\n",
       "      KAFKA_BROKER_ID: 1\n",
       "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
       "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n",
       "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Start the stack</strong></p>\n",
       "<pre><code class=\"language-bash\">docker compose up -d\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Check that containers are running</strong></p>\n",
       "<pre><code class=\"language-bash\">docker ps\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Test the installation</strong></p>\n",
       "<p><em>List topics</em>:</p>\n",
       "<pre><code class=\"language-bash\">docker exec kafka kafka-topics --list --bootstrap-server localhost:9092\n",
       "</code></pre>\n",
       "<p><em>Create a topic</em>:</p>\n",
       "<pre><code class=\"language-bash\">docker exec kafka kafka-topics --create --topic demo --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n",
       "</code></pre>\n",
       "<p><em>Produce/consume messages</em>:</p>\n",
       "<pre><code class=\"language-bash\"># Produce\n",
       "docker exec -it kafka kafka-console-producer --topic demo --bootstrap-server localhost:9092\n",
       "&gt; hello docker\n",
       "\n",
       "# Consume\n",
       "docker exec -it kafka kafka-console-consumer --topic demo --from-beginning --bootstrap-server localhost:9092\n",
       "</code></pre>\n",
       "<p>You’ll see the <em>hello docker</em> message.</p>\n",
       "</li>\n",
       "<li><p><strong>Stopping the stack</strong></p>\n",
       "<pre><code class=\"language-bash\">docker compose down\n",
       "</code></pre>\n",
       "</li>\n",
       "</ol>\n",
       "<hr />\n",
       "<h2>4.  Quick Tips &amp; Common Issues</h2>\n",
       "<table>\n",
       "<thead>\n",
       "<tr>\n",
       "  <th>Issue</th>\n",
       "  <th>Fix</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "  <td><code>NoBrokersAvailable</code></td>\n",
       "  <td>Make sure the broker is listening on the advertised address (<code>kafka.server.properties</code> → <code>listeners=PLAINTEXT://0.0.0.0:9092</code>). In Docker, check <code>KAFKA_ADVERTISED_LISTENERS</code>.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>“Connection timed out”</td>\n",
       "  <td>Verify that port <code>9092</code> is open and not blocked by a firewall.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>“Bootstrap broker list is empty”</td>\n",
       "  <td>The <code>bootstrap-server</code> flag may be missing or misspelled.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>“Zookeeper connection refused”</td>\n",
       "  <td>Ensure Zookeeper is up before starting Kafka (or set <code>KAFKA_ZOOKEEPER_CONNECT</code> correctly).</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Java <code>OutOfMemoryError</code></td>\n",
       "  <td>Increase <code>KAFKA_HEAP_OPTS</code> if you’re running many partitions.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Windows path issues</td>\n",
       "  <td>Use the Windows‑specific batch scripts under <code>bin\\windows\\</code>.</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<hr />\n",
       "<h2>5.  Next Steps</h2>\n",
       "<ol>\n",
       "<li><strong>Create more topics</strong> and experiment with partitions/redundancy.</li>\n",
       "<li><strong>Explore Confluent Schema Registry</strong> if you need Avro or JSON schema support.</li>\n",
       "<li><strong>Try the Kafka clients</strong> for your preferred language (Python, Java, Go, etc.).</li>\n",
       "<li><strong>Set up security</strong> (SSL/TLS, SASL) if you’ll run Kafka in production.</li>\n",
       "</ol>\n",
       "<hr />\n",
       "<h3>Clarifying Questions</h3>\n",
       "<ol>\n",
       "<li><strong>Operating system</strong> – Are you on macOS, Linux, Windows, or running inside a VM?</li>\n",
       "<li><strong>Preferred deployment</strong> – Plain Java binaries or Docker (or maybe Confluent Platform)?</li>\n",
       "<li><strong>Use case</strong> – Is this just for learning, or will you use Kafka in a production‑like environment?</li>\n",
       "<li><strong>Additional components</strong> – Do you need Schema Registry, REST Proxy, or other Confluent services?</li>\n",
       "</ol>\n",
       "<p>Let me know which direction you’d like to go, and I can tailor the instructions (or dive deeper into topics like topic configuration, replication, or security).</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "LookupError",
     "evalue": "('Please check model name. Use list_all_models function to see list of supported models.', LookupError(\"Unable to find provider with model matching 'openai/gpt-oss-20b'\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\toyaikit\\pricing.py:55\u001b[39m, in \u001b[36mPricingConfig.calculate_cost\u001b[39m\u001b[34m(self, model, input_tokens, output_tokens)\u001b[39m\n\u001b[32m     54\u001b[39m token_usage = Usage(input_tokens=input_tokens, output_tokens=output_tokens)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m price_data = \u001b[43mcalc_price\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_usage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_ref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CostInfo(\n\u001b[32m     58\u001b[39m     input_cost=price_data.input_price,\n\u001b[32m     59\u001b[39m     output_cost=price_data.output_price,\n\u001b[32m     60\u001b[39m     total_cost=price_data.total_price,\n\u001b[32m     61\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\genai_prices\\__init__.py:58\u001b[39m, in \u001b[36mcalc_price\u001b[39m\u001b[34m(usage, model_ref, provider_id, provider_api_url, genai_request_timestamp)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calculate the price of an LLM API call.\u001b[39;00m\n\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m \u001b[33;03mEither `provider_id` or `provider_api_url` should be provided, but not both. If neither are provided,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m \u001b[33;03m    The price calculation details.\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata_snapshot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_snapshot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcalc\u001b[49m\u001b[43m(\u001b[49m\u001b[43musage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_api_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenai_request_timestamp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\genai_prices\\data_snapshot.py:59\u001b[39m, in \u001b[36mDataSnapshot.calc\u001b[39m\u001b[34m(self, usage, model_ref, provider_id, provider_api_url, genai_request_timestamp)\u001b[39m\n\u001b[32m     57\u001b[39m genai_request_timestamp = genai_request_timestamp \u001b[38;5;129;01mor\u001b[39;00m datetime.now(tz=timezone.utc)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m provider, model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfind_provider_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_api_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model.calc_price(\n\u001b[32m     61\u001b[39m     usage,\n\u001b[32m     62\u001b[39m     provider,\n\u001b[32m     63\u001b[39m     genai_request_timestamp=genai_request_timestamp,\n\u001b[32m     64\u001b[39m     auto_update_timestamp=\u001b[38;5;28mself\u001b[39m.timestamp \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.from_auto_update \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     65\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\genai_prices\\data_snapshot.py:98\u001b[39m, in \u001b[36mDataSnapshot.find_provider_model\u001b[39m\u001b[34m(self, model_ref, provider, provider_id, provider_api_url)\u001b[39m\n\u001b[32m     96\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m provider_model\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     provider = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfind_provider\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_api_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model := provider.find_model(model_ref):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\genai_prices\\data_snapshot.py:128\u001b[39m, in \u001b[36mDataSnapshot.find_provider\u001b[39m\u001b[34m(self, model_ref, provider_id, provider_api_url)\u001b[39m\n\u001b[32m    126\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m provider\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUnable to find provider with model matching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_ref\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mLookupError\u001b[39m: Unable to find provider with model matching 'openai/gpt-oss-20b'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m messages = \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhow do I install kafka\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\toyaikit\\chat\\runners.py:492\u001b[39m, in \u001b[36mOpenAIChatCompletionsRunner.loop\u001b[39m\u001b[34m(self, prompt, previous_messages, callback, output_format)\u001b[39m\n\u001b[32m    489\u001b[39m             callback.on_function_call(function_call, content_val)\n\u001b[32m    491\u001b[39m pricing_config = PricingConfig()\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m cost = \u001b[43mpricing_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcalculate_cost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_input_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_output_tokens\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m token_usage = TokenUsage(\n\u001b[32m    497\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.llm_client.model,\n\u001b[32m    498\u001b[39m     input_tokens=total_input_tokens,\n\u001b[32m    499\u001b[39m     output_tokens=total_output_tokens,\n\u001b[32m    500\u001b[39m )\n\u001b[32m    502\u001b[39m new_messages = chat_messages[prev_messages_len:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\toyaikit\\pricing.py:73\u001b[39m, in \u001b[36mPricingConfig.calculate_cost\u001b[39m\u001b[34m(self, model, input_tokens, output_tokens)\u001b[39m\n\u001b[32m     70\u001b[39m     output_cost = (pricing[\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m] * output_tokens) / Decimal(\u001b[33m\"\u001b[39m\u001b[33m1000000\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CostInfo.create(input_cost=input_cost, output_cost=output_cost)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(\n\u001b[32m     74\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPlease check model name. Use list_all_models function to see list of supported models.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     75\u001b[39m     le,\n\u001b[32m     76\u001b[39m )\n",
      "\u001b[31mLookupError\u001b[39m: ('Please check model name. Use list_all_models function to see list of supported models.', LookupError(\"Unable to find provider with model matching 'openai/gpt-oss-20b'\"))"
     ]
    }
   ],
   "source": [
    "messages = runner.loop(prompt='how do I install kafka', callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d297392b-8234-4f71-9549-164cf9bfdc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "class SearchTools:\n",
    "\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "\n",
    "    def search(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search the FAQ database for entries matching the given query.\n",
    "    \n",
    "        Args:\n",
    "            query (str): Search query text to look up in the course FAQ.\n",
    "    \n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of search result entries, each containing relevant metadata.\n",
    "        \"\"\"\n",
    "        boost = {'question': 3.0, 'section': 0.5}\n",
    "    \n",
    "        results = self.index.search(\n",
    "            query=query,\n",
    "            filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "            boost_dict=boost,\n",
    "            num_results=5,\n",
    "            output_ids=True\n",
    "        )\n",
    "    \n",
    "        return results\n",
    "\n",
    "    def add_entry(self, question: str, answer: str) -> None:\n",
    "        \"\"\"\n",
    "        Add a new entry to the FAQ database.\n",
    "    \n",
    "        Args:\n",
    "            question (str): The question to be added to the FAQ database.\n",
    "            answer (str): The corresponding answer to the question.\n",
    "        \"\"\"\n",
    "        doc = {\n",
    "            'question': question,\n",
    "            'text': answer,\n",
    "            'section': 'user added',\n",
    "            'course': 'data-engineering-zoomcamp'\n",
    "        }\n",
    "        self.index.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e67c665-a017-4c4b-a600-cab29bb93ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tools = SearchTools(index)\n",
    "\n",
    "agent_tools = Tools()\n",
    "agent_tools.add_tools(search_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6ac715b-82c0-4071-9982-a30cad4cf45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'name': 'add_entry',\n",
       "  'description': 'Add a new entry to the FAQ database.\\n\\nArgs:\\n    question (str): The question to be added to the FAQ database.\\n    answer (str): The corresponding answer to the question.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'question': {'type': 'string',\n",
       "     'description': 'question parameter'},\n",
       "    'answer': {'type': 'string', 'description': 'answer parameter'}},\n",
       "   'required': ['question', 'answer'],\n",
       "   'additionalProperties': False}},\n",
       " {'type': 'function',\n",
       "  'name': 'search',\n",
       "  'description': 'Search the FAQ database for entries matching the given query.\\n\\nArgs:\\n    query (str): Search query text to look up in the course FAQ.\\n\\nReturns:\\n    List[Dict[str, Any]]: A list of search result entries, each containing relevant metadata.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'query parameter'}},\n",
       "   'required': ['query'],\n",
       "   'additionalProperties': False}}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_tools.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ea38b71-fd58-4669-9459-93373993de28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: howdy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "howdy\n"
     ]
    }
   ],
   "source": [
    "question = input('You:')\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50040f30-c4b7-481e-bded-ced484706ee7",
   "metadata": {},
   "source": [
    "## Pydantic AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32355ba2-32eb-40fc-abe5-3c76b296dbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bound method SearchTools.search of <__main__.SearchTools object at 0x000002305807A660>>,\n",
       " <bound method SearchTools.add_entry of <__main__.SearchTools object at 0x000002305807A660>>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "tools = [\n",
    "    search_tools.search,\n",
    "    search_tools.add_entry\n",
    "]\n",
    "\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf9de5e3-477d-421f-a837-171173b05f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=developer_prompt,\n",
    "    tools=tools,\n",
    "    model='groq:openai/gpt-oss-20b'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23c2b4cd-9d67-4286-be90-9f6cbd06ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await agent.run(user_prompt='how do I run kafka')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6cb11e-af3e-4c55-b3b2-3c75b702492f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fcb381a-6760-4102-b63b-8fef91a8b7c3",
   "metadata": {},
   "source": [
    "from toyaikit.chat.runners import PydanticAIRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b48299-9f21-41e3-89af-0cfc7ad132f8",
   "metadata": {},
   "source": [
    "runner = PydanticAIRunner(\n",
    "    chat_interface=chat_interface,\n",
    "    agent=agent\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80b66a-b5f2-4a44-81b9-3cae1a91429c",
   "metadata": {},
   "source": [
    "await runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d265225-443d-4846-9c9c-fe10d72f63b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
