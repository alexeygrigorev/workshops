Data-Centric AI

0:00 Hi everyone, welcome to our event. This
0:02 event is brought to you by data talks
0:04 club which is a community of people who
0:05 love data. We have weekly events and
0:08 this is one of such events. If you want
0:09 to find out more about the events we
0:11 have, please go to the description.
0:13 There is a link to our events page and
0:15 you will see all the events we have in
0:17 our pipeline.
0:18 Very important not to miss out.
0:20 Subscribe to our YouTube channel. So you
0:23 will get to see events like this one
0:25 today, conversations like this one today
0:28 and uh other fun ones. So yeah, don't
0:32 don't forget to subscribe and we have an
0:34 amazing Slack community where you can
0:36 hang out with other data enthusiasts. So
0:38 the link for registration is also in the
0:41 description. During today's interview,
0:43 you can ask any question you want. There
0:45 is a pinned link in the live chat. So
0:47 click on this link, ask your question
0:49 and I will be covering these questions
0:51 during the interview.
0:53 And this is the usual introduction I
0:56 have. Now it's over and I will open the
0:59 questions we prepared for you.
1:03 Did you like the questions?
1:06 Yes. Yes. I actually didn't see there
1:07 were questions until this morning, but
1:10 it's a topic that I really like to talk
1:12 about. So yeah, I was enthusiastic about
1:14 them.
1:15 Okay. So hopefully it's not too
1:18 surprising for you what you saw there.
1:21 Okay. Uh, you ready to start?
1:24 Yes, of course.
1:26 So, this week we'll talk about
1:27 datacentric AI and we have a special
1:30 guest today, Marishia. Marishia works as
1:32 a lead data scientist at Go Data Driven.
1:34 She has a strong interest in education
1:36 and teaching both as a part of your
1:39 current role at Go Data Driven and also
1:41 as a co-organizer of PI data Amsterdam
1:44 and PI data global. Welcome.
1:46 Yeah, nice. Thank you for having me.
1:49 The questions for today's interview were
1:51 prepared by Johanna Byer. Thanks Johanna
1:53 for your help. So before we go into our
1:56 main topic of data centric A, let's
1:59 start with your background. Can you tell
2:00 us about your career journey so far?
2:03 Yeah, sure. Um, so I started by studying
2:07 artificial intelligence at the
2:08 University of Amsterdam. I did both a
2:10 bachelor and a master in artificial
2:11 intelligence and my early career was
2:14 focused on specifically applying deep
2:16 learning on medical imaging particularly
2:19 early stage lung cancer detection and 3D
2:21 CT scans and within that domain my focus
2:25 was on geometric deep learning on the
2:27 medical domain which was also the topic
2:29 of my master thesis supervised by Max
2:31 Benning. uh but after that I
2:33 transitioned into the role of data
2:35 science educator at go data driven which
2:37 meant that I taught and created courses
2:40 on basically all things data science and
2:43 now I work as a lead data scientist at
2:45 go data driven and in addition to that I
2:48 organized meetups and conferences mostly
2:50 in and around Amsterdam with data.
2:55 What did you do as data science
2:56 indicator? You said uh you your
2:59 responsibilities included creating
3:01 courses.
3:02 Yes,
3:03 that's cool.
3:04 Yeah. Yeah. So at the go data driven
3:06 academy we teach a lot of courses on
3:08 everything data science. So some of are
3:10 very generic like an introduction to
3:12 Python for data analysts or an
3:15 introduction to uh data science for
3:17 instance. But we also create a lot of
3:20 courses very specifically targeted
3:21 towards a specific audience. So for
3:23 instance, I created a deep learning with
3:25 NLP course or a an unsupervised learning
3:28 course and those are more detailed or
3:30 more specific topics and it gave me an
3:32 opportunity to really dive into that
3:34 topic and create good exercises and
3:36 assignments and material on that. That
3:38 was really fun. Yeah, I think I spoke
3:41 with um folks from your company from go
3:43 data driven at the recent PI data pyon
3:46 in Berlin and uh as far as I remember
3:50 you're doing education and consultancy
3:52 right that's
3:54 yes so I was mostly a data science
3:57 educator for about two years but I
3:59 strongly believe that you can't be a
4:01 good teacher if you don't also have
4:03 hands-on experience so I I like to
4:06 really when I do courses and when I
4:07 teach courses really tell a lot of
4:10 anecdotes about my experiences and in my
4:13 work because it speaks more to the
4:14 imagination of why we're doing this than
4:16 just talking about the concepts. Uh so
4:19 while I really enjoyed it, I do feel
4:20 like after two years mostly focusing on
4:23 the educational side, I I need some
4:25 hands-on experience and also I was
4:27 really missing just the coding bit. So
4:29 that's so so the ideal situation for me
4:32 is to do both trainings and education
4:34 and also work as a data scientist and
4:37 combine that in some point.
4:39 Do you still u teach?
4:42 Uh occasionally some courses but I'm
4:44 more focused on my lead data science
4:46 role at the moment.
4:46 Mhm. And what do you do as a lead data
4:49 scientist?
4:50 Um I'm with a company where um I'm
4:53 mostly focused on building a community
4:55 of practice there. They just went
4:57 through a transition in their the way
4:59 that they organize their teams and I
5:01 want to make sure that all the data
5:02 scientists still communicate clearly
5:05 with each other get to exchange
5:06 knowledge but also increase the maturity
5:08 level of the data science products that
5:10 we produce. So make sure that we are not
5:13 just everyone doing something on their
5:15 own time behind their own laptop but
5:17 bringing them together make sure that we
5:18 actually get to mature well functioning
5:21 monitored data science products.
5:24 Yeah, this is such an interesting topic.
5:25 Uh but I'm afraid our I want to ask more
5:29 about that maybe at the end if we run
5:30 out of questions. Uh because the the
5:33 main topic for today is actually
5:35 dataentric
5:36 and maybe it's related to what you do
5:40 right now building a how you say
5:42 building a community of practice and um
5:46 improving maturity. So let's go back to
5:49 datacentric. So what is what is
5:51 datacentric? Why do we care about this?
5:54 Yeah, that's a that's a good question. I
5:56 did a whole talk at PI data London about
5:58 this whole the answering those
6:00 questions, but basically in short, the
6:02 central idea behind datacentric AI is
6:05 that the focus has to shift from big
6:07 data to good data. So Andrew said uh
6:11 that having 50 thoughtfully engineered
6:14 examples can be sufficient to explain to
6:16 the neural network what you wanted to
6:18 learn. And the reason why we call it
6:20 datacentric AI is because it's in
6:22 contrast to the model centric approach
6:24 that a lot of us uh are used to because
6:28 in a model centric AI the focus is
6:31 really on iterating on the model which
6:33 means that you create a baseline model
6:35 given the data that you have you
6:37 evaluate the baseline and then you go
6:39 back to the model you revisit the
6:41 modeling pipeline and you make
6:43 adjustments there. You change the
6:45 algorithm. You adjust the architecture
6:46 in your neural network. You tune
6:48 hyperparameters. Maybe even adjust some
6:51 of the data transformations such as how
6:53 you impute missing values or augment
6:56 your images.
6:58 But the data is generally considered as
7:00 or the data set is generally considered
7:02 as static. And the idea behind data
7:04 centric AI is that we shouldn't just
7:07 iterate on the model, but we should
7:09 iterate on the data as well. And that
7:12 means improving the quality of our data
7:14 by relabeling uh mislabeled or ambiguous
7:17 data points. But can also mean gathering
7:19 more data or more examples of specific
7:22 classes or readjusting the train and
7:24 validation split.
7:26 Uh so dataentric AI is essentially about
7:28 focusing on your data rather than just
7:31 focusing on your model. And the idea of
7:34 that is of course isn't new. I mean I
7:36 know that one of the first things that I
7:38 was taught was garbage in garbage out.
7:39 So that's always been the idea. But the
7:42 thing is that I think or I strongly
7:44 believe that dataentric AI is
7:46 particularly relevant now uh than
7:49 compared to let's say for example 10
7:51 years ago and it's like the hype around
7:55 deep learning. Um deep learning wasn't
7:58 new in 2010 for instance the ideas
8:01 weren't new. Perceptron we know about
8:03 perceptron since the 50s. The back
8:05 propagation has been around since the
8:06 80s. Even the first convolutional
8:08 networks were already around in the
8:10 '90s. But somehow deep learning really
8:14 came to prominence since let's say about
8:16 2010 onwards. And that's not because the
8:19 ideas themselves were new, but because
8:21 the ground had changed. So we had large
8:24 annotated data sets available, GPUs that
8:27 became available, which meant that deep
8:28 learning gained more traction.
8:31 And I believe that something similar is
8:33 happening for datacentric AI. We've
8:36 always known that data is important but
8:39 it's more important now. Um for I
8:42 believe two main reasons. Uh first of
8:44 all because more and more problems are
8:47 now being solved with deep learning. We
8:49 are more often dealing with unstructured
8:51 data rather than structured tabular
8:54 data. So unstructured data like uh
8:57 images or audio or text.
9:00 And whereas for tabular data, it's
9:02 easier to focus on the quality because
9:04 you have descriptive statistics and you
9:06 can easily create visualizations to
9:09 investigate correlations between
9:11 features. You it's it's easier to
9:14 explore your data and it's also easier
9:16 to get an idea of the data and before
9:18 you start any modeling. So after some
9:21 proper exploratory data analysis,
9:23 focusing on good data quality is kind of
9:25 a natural result of that.
9:28 But this is remarkably more complicated
9:31 when you get to unstructured data um
9:33 because it's more difficult to get a
9:35 good insight of the data just you have
9:37 at hand with unstructured data. You can
9:39 sample a few images but do you know for
9:42 sure that your data is good and
9:44 representative.
9:46 So because we're dealing with
9:48 unstructured data now more than ever it
9:50 we are more in need of toolings and
9:53 techniques to help us out with that.
9:56 It's a bit counterintuitive to me
9:58 because now we have I mean that we need
10:00 this datacentric AI now more than 10
10:03 years ago because it seems like 10 years
10:05 ago when we didn't have all these GPUs
10:07 we needed to be smart about how we
10:09 approach things right and then it
10:11 mattered like what kind of data we have
10:13 but now you just take a I don't know a
10:17 cluster of GPUs throw in more data
10:20 and like you sit back and wait till it
10:23 magically becomes
10:25 Uh it doesn't work like that, does it?
10:28 No, unfortunately not. There's this
10:30 persistent idea that you know the
10:32 quantity of the data will compensate for
10:34 the quality. So if your data quality is
10:36 not good, just gather a thousand more
10:39 examples and that's fine.
10:41 But I think something really changed
10:42 because we're more than ever making use
10:44 of transfer. So if you have to train a
10:47 neural network all the way from scratch,
10:49 then yes, more data is probably a good
10:52 way to go. Um but nowadays we have large
10:55 foundation models um and we are mostly
10:58 focused on fine-tuning those because I
11:00 as like an individual data scientist I
11:02 don't have the resources to to compete
11:04 with something like GPT3 so I fine-tune
11:08 it to my own problems and when you're
11:10 fine-tuning that's the situation where
11:12 the model has already learned a lot
11:15 about the structure of images the
11:17 structure of text but you want to
11:19 fine-tune it to your specific problem.
11:21 So if in that situation you are giving
11:23 it examples that aren't right
11:26 then it's basically fine tuning on the
11:28 wrong thing and I think that's one of
11:30 those cases where the data matters more
11:33 the quality of the data matters more and
11:35 it's also one of the cases where we as
11:38 data scientists can have a bigger impact
11:40 because I'm not going to adjust the
11:42 architecture or the parameters or the
11:44 the of a large model that is provided to
11:48 me a foundation model but I can I know
11:50 about my data. I know about my use case
11:52 and I can focus on the data more than I
11:55 can focus on the model itself.
11:58 I'll try to recap everything you said
12:00 maybe not everything but the main idea.
12:02 So we have two approaches datacentric
12:05 approach and model ccentric approach. So
12:07 in the modelcentric approach the data
12:09 set is static and you iterate on the
12:11 model maybe tune the model change
12:13 parameters try different architectures
12:15 you make some adjustments you might also
12:18 do some feature engineering right but
12:20 the data set the images or the rows of
12:23 your data set they remain the same.
12:28 In contrast to that in datacentric
12:29 approach
12:31 um you change the data instead of
12:34 changing the model or in addition to
12:35 changing the model. So you also u look
12:38 at the data you see the bad examples you
12:40 see like the good examples you see where
12:42 you can improve the data instead of
12:44 focusing on the model you put more
12:46 effort more emphasis on the second part
12:48 on the data part
12:50 right?
12:51 Yep.
12:51 And I think that this modelcentric
12:53 approach is very typical for a Kaggle
12:55 competition. So in a kaggle competition
12:57 you have a train csv file right and then
13:01 you have test csv file and then that's
13:04 all you got right and then you of course
13:06 have some room for experiments like you
13:09 can uh I don't know tune your xg boost
13:13 model you can train as many models as
13:16 possible you can put all of them
13:17 together in an an assemble but then it's
13:20 model ccentric approach right because
13:21 you're tuning the snops of the model
13:25 while in data centric approach it's
13:27 different and the reason I spoke about
13:30 Kaggle because I also heard that there
13:32 are competitions about datacentric AI
13:36 and to my knowledge you even took part
13:38 in one of or maybe multiple ones one of
13:41 them so maybe can you tell us about
13:44 these competitions
13:45 yeah sure um yeah so of course I took
13:48 part in multiple kag competitions as
13:50 many data scientists as well but I
13:52 really recognize what you're saying that
13:53 usually
13:54 you you have your data and you don't go
13:56 about gathering more data or it's it's
13:59 more about the model itself. Um but as a
14:02 contrast to that uh we have the
14:04 datacentric AI competition that I
14:05 participated in with two of my
14:07 colleagues dimmed and Roberto and the
14:10 datacentric AI competition was a
14:12 competition hosted by deep learning.ai
14:14 AI that ran for 6 weeks in September
14:17 2021 and the central idea behind this
14:20 competition was the model was fixed and
14:24 the data could be changed. So the task
14:27 was about classifying images of Roman
14:29 numerals, handwritten Roman numerals.
14:32 And we were initially provided with a
14:34 data set of 3,000 images divided into a
14:36 certain train and validation split. And
14:40 it was up to us to submit the data and
14:43 all the compute was handled on the
14:45 channel side. So the model was a fixed
14:47 dresset 50 and we could submit up to
14:50 10,000 images. So there was a cap there.
14:52 we can just say let's uh let's just get
14:55 a huge quantity of data.
14:57 That's smart to do at this limit, right?
14:59 Because you can just generate so much so
15:01 many images and just overload the system
15:05 with this.
15:06 Exactly. So there was a cap on that and
15:08 one of the things that I really liked
15:10 about that besides you know introducing
15:12 me to this idea of datacentric AI was
15:14 that um it made participating very
15:17 accessible. It was a deep learning
15:19 challenge because it was an image
15:20 challenge, but you didn't need any beefy
15:23 GPUs uh because all the compute was
15:25 handled for you. So that means that
15:27 anyone with about 10 MB of storage space
15:30 and an internet connection could
15:31 participate
15:33 and like I said I participated with my
15:35 colleagues Dimal and Albertans and we
15:37 ended up winning in the most innovative
15:40 category and presenting our solution
15:43 at the datacentric AI workshop at Nurips
15:46 alongside the other women. That's really
15:48 great. Was it your first exposure to
15:50 this to this idea of datacentric AI?
15:53 Yeah, that's an interesting question.
15:55 So, it was my first exposure to the
15:57 concept. So, I'd not heard of
15:59 datacentric AI before, but my one major
16:02 competition that I participated in
16:04 before was also a Kyle competition and
16:07 that was the data science bowl um where
16:09 we participated in detecting whether
16:12 someone had lung cancer or not based on
16:15 CT images. And that wasn't a datacentric
16:18 AI competition, but it turned out we
16:20 ended up in third place, me with the
16:22 company that I was with at the time. Um,
16:25 but it turned out that all the top three
16:27 solutions, they really didn't use the
16:30 data that was provided by KGO
16:31 whatsoever. Um the second solution by
16:35 Julian Dit was all focused on creating a
16:38 system so he could easily evaluate
16:41 whether the scans or the data uh that he
16:44 had was any good and making selections
16:46 based on that and creating a tool that
16:48 allowed him to easily annotate
16:50 additional information which made it
16:53 easier for him to to get to that second
16:55 place solution. So while that wasn't
16:58 named as a datacentric AI competition,
17:01 it was very interesting to me that we
17:02 all all the top solutions ignored the
17:05 data that we were provided with and
17:06 decided to focus on uh gathering
17:09 additional data, creating a model in a
17:12 completely different way based on the
17:13 data that we did think was more useful.
17:16 So was kind of data centric in that
17:18 sense as well.
17:20 You just didn't know it was called
17:21 datacentric.
17:22 Yeah, exactly. Mhm. It's pretty typical
17:25 for Kaggle competitions, I think, to use
17:27 external data. You you'll have to
17:30 disclose that you use this particular
17:32 data set, right? But uh I I guess it's
17:36 it gives you an edge in uh image
17:38 competitions when there is an external
17:40 data set that you can use to make your
17:43 model better.
17:44 Yeah, I agree with that. But I also
17:46 think that there's a difference between
17:47 simply gathering more data and for
17:50 instance what some of the winning
17:51 approaches were based on focusing on
17:54 finding out what the most useful data to
17:57 add was like what what things were we
18:00 missing in our data what things weren't
18:01 we missing which examples had to have a
18:04 higher weight because they were more
18:06 important than other data samples. those
18:08 kinds of decisions about your data which
18:10 I think goes a bit further than just get
18:14 getting some extra images.
18:17 That's this is a nice answer to a
18:19 question why should I take part in data
18:21 science competitions right then you
18:23 might stumble into an idea and then uh
18:26 for you you said it was September 2021
18:29 not so long ago but now you are giving
18:32 talks about datacentric you're talking
18:34 on podcasts about that so I guess your
18:36 life and then you also won in this most
18:39 innovative uh approach award right so I
18:42 guess your life changed a little bit
18:44 after taking part in that competition
18:46 I think datacentric AI for me is I mean
18:50 when we talk about something like this
18:51 it's very often focused on the tools and
18:53 the methods like how do we do something
18:56 like this what like what kind of
18:57 packages do you use what kind of tools
18:59 but for me the most important thing was
19:01 a mindset shift focusing on the data and
19:05 not seeing the data sets as static has
19:08 really helped me throughout my my career
19:11 since that moment because I think that's
19:12 a very important insight that yes I may
19:16 this may be the data that I have but I
19:19 can also make decisions about that and
19:20 change it throughout my modeling
19:22 process.
19:24 Yeah, that's interesting the point about
19:26 changing. So earlier today you mentioned
19:29 that your train validation split is also
19:32 it doesn't have to be static right and
19:35 then uh immediately to me uh like I
19:38 thought but wait if our validation set
19:40 is not static how do we compare two
19:42 approaches and say that this approach is
19:44 better than this one if our validation
19:47 set is inconsistent if we change it
19:49 between two runs you you see what I mean
19:51 right so like you have let's say two
19:53 models right and then usually the the
19:56 easiest way to compare these two models
19:58 is to evaluate these models on the same
20:00 validation data set and whatever model
20:03 gets higher score is better.
20:05 Mhm.
20:05 Right. But the moment we change
20:07 validation data set, we cannot compare
20:10 these models because they are evaluated
20:12 on different data sets. Right? And for
20:14 for me
20:16 it got me thinking like okay like now if
20:18 we start changing the validation data
20:20 set, how can we be sure that it's
20:23 actually an improvement?
20:25 I think that's a very valid question. Uh
20:27 first of all, when we were participating
20:31 in the datacentric AI competition, I
20:33 think that our insight that the train
20:34 and validation split that we were
20:36 provided with wasn't um for us a good
20:39 split. Our validation set turned out to
20:41 be not very representative of all of the
20:43 data. There was a huge part of the data
20:46 that was not represented in the
20:47 validation set that was represented in
20:49 the train set. So we decided to
20:51 rebalance that. In our case in the
20:53 datacentric AI competition, we did not
20:55 have access to an actual test set
20:57 because that was handled on the
20:59 competition side. So there was another
21:01 hold out set that we were eventually uh
21:03 evaluated on and the validation set in
21:06 this case was used um in order to
21:08 determine when we were done training. So
21:11 the for early stopping was when it was
21:13 used. Um so in that sense it still
21:17 matters. I do believe that
21:22 I have a I have a very technical
21:23 artificial intelligence. I like to focus
21:25 on the numbers type of background. But I
21:27 do also believe that we shouldn't only
21:29 focus on the number of the metric.
21:31 Sometimes you can make a change to your
21:34 test set to your validation set which
21:36 makes the numbers go down but gives you
21:39 more confidence that that number is
21:41 correct. If you notice that for instance
21:43 your test set or your validation set,
21:45 the thing that you eventually validate
21:46 on um is missing a part of the data that
21:51 you do expect to encounter in practice.
21:53 Do you not add that data to your test
21:55 set because it's no longer you're not no
21:57 longer able to compare the bottles? Do
21:59 you not add it because your metric will
22:02 go down? I think it's better to change
22:04 it but be confident about the change
22:07 that you have made. that makes it more
22:08 trustworthy what your eventual result
22:11 will be.
22:13 And then at the end you can just reulate
22:15 all this. Um I know we're talking about
22:17 dataentric approach not model ccentric
22:19 but then at the end if you change your
22:20 validation data set you can just
22:22 reevaluate your approaches on the new
22:24 split.
22:25 Exactly.
22:26 Yeah. I I think I also want to emphasize
22:28 of course that datacentric AI doesn't
22:31 mean that we shouldn't change the model.
22:32 I think it's it's often um named in
22:35 contrast to model ccentric AI, but for
22:37 me datacentric AI means that we iterate
22:40 on both the model and the data. So yes,
22:43 doesn't mean that I just take a baseline
22:45 and never change my model anymore. Um
22:47 because in practice that won't get me
22:48 the best results, but I do think there's
22:50 in very very often there's more to get
22:53 from improvements on the data than for
22:56 instance change the entropy in your
22:58 decision tree to a genie for instance.
23:01 Yeah. Right. Um well you said for you
23:05 the most important realization was to
23:07 the mindset shift from not just how we
23:10 do this but also that you know data set
23:13 is not a static thing you can change it
23:16 but I'm still wondering how do we
23:17 actually do this like what are the tools
23:20 what are the approaches uh for that how
23:23 do we implement this
23:24 yeah that's a very good question um so
23:27 there's not one toolbox that I can
23:30 recommend that has everything I think
23:31 it's very broad subject. It's it's also
23:33 what you like to focus on. There's a lot
23:35 of tools out there that can help you
23:37 with labeling
23:39 um or finding the the the data points in
23:42 your data that need reabling. But of
23:44 course, there's different tools for
23:45 textbased or image based or audiobased.
23:49 But there's way more to dataentric AI
23:50 than just reabeling. There are also
23:52 tools for um for instance generating
23:56 synthetic data and how do you create
23:57 good synthetic data to augment your your
24:00 current data set. So there's a very
24:03 broad spectrum of things that are all
24:05 data related. Um and there's also a lot
24:08 of development at the moment being done
24:10 on these tools because I think that at
24:12 the moment we all experience especially
24:14 working with uh foundation models,
24:16 working with unstructured data, we
24:18 experience a need for tools that help us
24:20 out with these kinds of things. So a lot
24:22 is currently being developed. Um a lot
24:26 of high-tech tools, a lot of low tech
24:28 tools as well. I'm a personally a big
24:31 fan of
24:33 not using one thing and having
24:35 everything work out of the box. I
24:36 wouldn't like the magic fix all your bad
24:39 labels tool that there would if there
24:42 would be one because I think that I have
24:45 my value as a data scientist is in
24:47 understanding the data and talking with
24:49 subject matter experts. So I I like to
24:51 have a lot of control over that process.
24:54 Um, and I think the most important
24:56 lesson that we learned is that it at the
24:59 end it's not about what tools you use,
25:01 it's about how easy you make it for
25:03 yourself to um to iterate on the data
25:07 and how you keep track of that. So that
25:09 could mean that you use DVC to version
25:11 your data and you have a good overview
25:13 of all the data sets that you've used.
25:15 But in our case for the datacentric AI
25:17 competition, we were still naming things
25:19 with underscore version three, which
25:22 maybe wasn't the nicest versioning
25:24 approach, but it was a easy for us to
25:26 releabel our data and that was a very
25:28 important thing that wasn't the
25:29 bottleneck.
25:31 Yeah, funny that you mentioned the
25:33 approach uh for data versioning that you
25:35 had in data talks club. We recently
25:37 launched a competition and this
25:39 competition is um about predicting not
25:43 detecting determining classifying images
25:45 of different kind of kitchen stuff like
25:48 it's a plate cup glass uh forks. I
25:52 think I saw that. Yes.
25:53 Yeah. Yeah. And for me like I was
25:57 preparing the data for this competition
25:59 and the folder that I ended up with um
26:02 the name was uh like new to Kaggle final
26:05 something.
26:07 Yeah, I think we all
26:08 that was terrible. Yeah. Uh I wish like
26:12 maybe for the ne next one I'll use
26:14 something like one of these tools that
26:15 you mentioned like DVC or something else
26:18 because it was like very hard to keep
26:19 track at the end like what was changed
26:21 between like these thousands of folders.
26:24 Yeah.
26:25 Yeah. It's the same thing when I started
26:27 out as a data scientist when I was was
26:30 uh trying out different hyperparameters
26:32 I would be writing things on a post-it
26:33 note next to my laptop you know and
26:35 there's a better
26:36 better way to track your experiments
26:38 than just writing everything on a
26:40 post-it note I think the same goes for
26:41 your data probably
26:43 um so yeah that's versioning but also
26:45 just changing your data I think that's
26:47 an important thing for a lot of people
26:48 it's it's difficult to do because
26:50 there's a whole it's considered static
26:54 so one of the things that made it really
26:56 easy for us during the datacentric AI
26:58 competition is that we basically started
27:00 labeling in Google Docs. We found out
27:02 that when you have the URL of an image,
27:04 you see the image itself. So we could
27:06 very easily change it uh change the
27:08 label that was associated with it and
27:10 then turn it back into a pandas data
27:12 frame and then do some magic scripts so
27:15 that every file would be in the right
27:16 folder. And it took a bit of time to
27:18 create those scripts, but it made all of
27:20 the other work a lot easier for us in
27:22 the long run to adjust things. And I
27:24 think that's an important one. Make it
27:25 easy to make adjustments.
27:27 Mhm. So the process you had, you had a
27:30 Google spreadsheet with u one column I
27:34 guess URL, another column class, right?
27:37 And then you could just go there and
27:40 change a label there or multiple labels
27:42 and then you had a script that would
27:44 pull data from this Google spreadsheet
27:46 and train a model and then it would say
27:48 okay like
27:48 exactly
27:49 for this uh for this version of the
27:51 spreadsheet this is the score you have.
27:54 Uh yeah exactly. Uh we also had some
27:57 little tricks to make it easier for us
27:59 to work with the spreadsheet because
28:00 again there were 3,000 images so that
28:02 means that you have 3,000 rows. So we
28:05 did things like um which was relatively
28:07 easy to do um put the data through a
28:10 model already get some predictions and
28:12 then um order the data points on
28:15 confidence of the model for instance or
28:18 we made extensive use of the embeddings
28:20 visualizing the embeddings and seeing
28:22 that some uh that some data points were
28:25 very far away from the distribution of
28:27 that class and then that's one of those
28:29 data points that you pay attention to.
28:31 So we made use of little tricks like
28:33 that to to filter on what to focus our
28:35 attention on.
28:37 And I recently had a chat with one of
28:39 your friends P data colleagues Vincent
28:42 Vincent Barbadam
28:44 and I think he's into really into these
28:46 tools that help you with uh
28:49 finding bad data, right?
28:51 Yes, that's true. He uh he actually did
28:54 a talk in P data anov last week on bulk
28:57 labeling with a lot of tricks which was
28:59 a really good talk that I recommend.
29:02 No. So maybe you said like the most
29:05 important thing is focusing on the
29:07 approach, how iterate, how you iterate
29:10 over this and how you make it easier for
29:12 for you to iterate rather not um instead
29:17 of focusing on high-tech and low tech
29:20 tools. So I guess the the low tech tool
29:22 that you used in your competition was
29:24 this Google uh spreadsheets. Yes. Right.
29:27 But I'm really curious about the
29:28 approach that you
29:31 took like how would you actually
29:33 implement this in practice? How would
29:35 you let's say you join an organization
29:38 as a consultant or maybe as a data
29:40 scientist inhouse data scientist and you
29:43 want to follow this datacentric AI
29:45 approach.
29:46 How would you structure your project?
29:48 What kind of tools would you use to make
29:50 it easy to implement all the things we
29:53 discussed?
29:55 That's a good question. I don't think I
29:57 have a specific answer on what tools I
30:00 would use, but that's I think mostly
30:01 because I'm not I don't feel strongly
30:04 about certain tools over others. I'm
30:06 more interested in the process. I think
30:07 one of the most important changes that I
30:09 would make and I have made in the
30:11 organization that I'm with is that one
30:14 of the most important lessons that I
30:15 learned from the dataentric AI
30:17 competition is that data is more easy to
30:19 talk about than the model is. It's very
30:22 hard to go to subject matter experts and
30:24 talk about your results and say, well, I
30:26 used weight normalization instead of
30:27 batch normalization. That doesn't that's
30:30 not a very viable conversation. But you
30:33 can show examples of the data. You can
30:36 talk about the data. You can talk about
30:38 the odd examples that you find and go to
30:41 someone who knows more about the source
30:43 of the data that can explain things to
30:45 you. And I think this was particularly
30:47 relevant when I worked in the in a
30:49 medical imaging company where we
30:50 actually had a doctor employed who
30:53 whenever I found odd things in the data
30:55 I could go and talk to him, show him and
30:58 he would explain things to me and that
31:00 would really adjust the way that I
31:01 approach the modeling as well. So I
31:04 wouldn't have any specific tools to
31:06 recommend but I would recommend having a
31:08 very close connection to the person who
31:10 knows more about the data and realize
31:12 that when you focus on change if you get
31:14 the same performance but changing the
31:17 data or by changing the model by
31:19 changing the data it's much easier to
31:20 collaborate and it's also much easier
31:22 for the person who you're presenting the
31:24 model or the end result to to have a bit
31:26 of faith that it's it's working
31:28 correctly rather than just an abstract
31:30 metric.
31:34 I guess what I wanted to hear from you
31:36 was uh more
31:39 how to say tactical like for for me what
31:42 you say sounds like strategy. Okay, you
31:44 need to be close with subject matter
31:45 experts which is super valid but I'm
31:48 still wondering like how do I actually
31:50 make it happen? So I have a project I
31:53 have a bunch of subject m matter experts
31:55 and then I have a data set and I want to
31:58 make sure that I don't go crazy. I don't
32:01 have like a thousand uh folders with
32:03 names like new, version two, Kaggle
32:06 final, new and so on. Like how do I make
32:09 it happen? Like how do I organize this?
32:12 Um do you have like any tips on tricks
32:14 or best practices or I don't know talks
32:17 that I should check or anything like
32:20 that?
32:21 Um yeah there's a there's actually um
32:24 okay the reason why I find it very
32:26 difficult to give an answer to this is
32:27 because I think there's a lot of great
32:28 tools out there but there's two
32:29 resources there's two resources that I
32:32 find very useful one is by hazy research
32:34 and one is by y data they have a really
32:36 good overview of awesome AI datacentric
32:39 AI tools that are um structured in the
32:42 way is it about profiling is it about
32:44 synthetic data is it about do you are
32:46 you working with images are you working
32:48 with text because the tools are very
32:49 specific to that So those are two
32:51 resources that I would recommend to look
32:53 for the right tools for the use case
32:54 that you're working with and I would as
32:57 a data scientist still start with a
32:59 model centric approach. I would still
33:00 create a baseline as a model but then
33:03 use those model results to not only go
33:05 back to the uh to the model and how to
33:07 adjust those hyperparameters but also
33:10 use the results of that to see if
33:12 there's any gaps in my data that I'm
33:14 that I'm seeing. So it's basically doing
33:17 error analysis and understanding where
33:19 the model was wrong and then trying to
33:21 understand why the model was wrong and
33:23 maybe not maybe but talk to people who
33:27 know data well
33:28 to
33:29 exactly
33:30 figure this out because maybe for you
33:31 alone it could be difficult to
33:34 understand why for this particular data
33:37 set this was the final label or this is
33:40 this was the predictions right so maybe
33:41 it's it helps to talk to subject a
33:44 subject matter expert to figure this out
33:46 and maybe conclude that maybe the label
33:49 on this example is actually not right
33:51 and it should be a different one.
33:54 Okay. Exactly.
33:54 So um so this is how the process looks
33:57 like, right? So you train a model, you
33:59 analyze the um you analyze the errors,
34:02 you analyze the mistakes of the model,
34:04 you talk to subject matter experts and
34:06 you iterate iterate iterate until the
34:08 model is good enough.
34:11 Yeah, basically. Okay, sounds simpler
34:14 than I thought.
34:16 I think it's I think it's always
34:18 important in these kinds of things to
34:19 keep humans in the loop. And that could
34:21 be subject matter experts, but that
34:22 could also be just a data scientist who
34:24 who's learned obviously a lot about the
34:26 data as well and knows what they're
34:28 doing. I'm I'm not a big fan of the type
34:30 of tools that automate everything away.
34:32 I know there's a lot of administration
34:33 going on that can really help us, but
34:35 always keep a human in the loop to be be
34:37 sure that you can can truly trust your
34:40 results. Mhm. But I must admit it sounds
34:43 terribly similar to
34:47 standard data cleaning. U like you have
34:51 u you have errors then you go to the
34:53 data set and you see okay this row
34:55 doesn't make sense this is an outlier I
34:57 just throw it away. And then maybe you
34:59 even have a rule that okay if like this
35:02 uh value in this feature in this column
35:05 is like two sigas away from the mean
35:07 then you just throw it away or you add
35:10 it or whatever which is a pretty
35:12 standard data cleaning step
35:16 probably
35:17 um like what's the difference between
35:20 these two approaches or the data
35:21 cleaning is datacentric I
35:24 think data cleaning is a part of
35:25 datacentric AI but datacentric AI itself
35:27 is more broad it's easiest I guess to
35:30 talk about datacentric AI in the terms
35:32 of what is a good label and what is a
35:34 bad label or how do you choose to deal
35:36 with your missing values um but it's a
35:38 lot broader than that It's also for
35:40 instance about is my what I think is a
35:42 very important thing is my data set
35:44 representative and is there any bias in
35:46 that is my data set complete and those
35:49 are not things that are typically part
35:50 of data cleaning and there are tools
35:52 being developed for instance uh for
35:56 images to see if you have a
35:58 representative data set if you have a
36:00 complete data set and those tools can
36:01 really help there as well
36:04 how do we actually check that I'm really
36:06 curious like I have a data set now with
36:08 I know spoons forks, cups, glasses, like
36:11 how do I know if it's complete?
36:14 Yes, that's a a good question. I think
36:18 it's very hard to know if you just
36:21 without any domain knowledge. Um,
36:25 so for example, one of the one of the
36:27 approaches that one of a toy project
36:30 that I did once was classifying
36:31 penguins, classifying penguin species
36:33 based on images. And I sourced the data
36:35 set basically just through um
36:37 downloading all the Google image
36:39 results. And of course lots of the data
36:40 is right. There's a few mistakes in
36:42 there. And it was easy to focus on
36:45 gathering those bad labels out of there
36:47 and and reabeling those. But I think
36:49 that's one of those cases where it's
36:50 very easy to um I mean if if there's one
36:55 penguin classified wrong, maybe we can
36:57 gather a bit more data and that kind of
36:58 over compensates for that, right? But
37:02 to do the to make sure that it was
37:04 representative, I um in that particular
37:08 in that particular case, I thought about
37:10 what situations are there where I can
37:13 encounter penguins like they can be on
37:15 land, they can be in the snow, they can
37:16 but they can also be on water for
37:18 instance and I need to have groups of
37:21 penguins and I need to have uh
37:23 individual penguins and I need to have
37:25 penguins from the side and maybe not
37:27 necessarily from the top but you know
37:29 all the things that I can encounter.
37:31 And I noticed just by going through my
37:33 data that I was missing a lot I didn't
37:36 have a lot of example of baby penguins
37:37 for the for the different species for
37:39 instance. So that was one of those
37:40 examples where my data set was not
37:42 complete. And of course this is
37:44 something that I could have figured out
37:46 by just scrolling through all the images
37:49 and noticing this. But in this
37:51 particular case I came up with the
37:53 different things that I thought I should
37:54 have in my images and I decided to
37:58 visualize the embeddings of my images.
38:00 So I um basically put my data through a
38:03 neural network that was already trained
38:05 and didn't take the the head of it but I
38:07 took the embeddings and I reduced the
38:09 dimensionality to UMAP with UMAP so I
38:13 could visualize it and I use an
38:14 interactive tool um to be able to view
38:18 my images and I saw different clusters
38:20 of types of images because of course all
38:23 the penguins in the water were kind of
38:25 together in terms of embeddings and all
38:27 the penguins on land were kind of
38:28 together and I used that interactive
38:31 tool to get a bigger insight in my uh
38:34 data. And then I noticed yes, I did see
38:36 a few baby penguins, but I didn't see a
38:38 lot of data points around those. There
38:40 were only three in my data set. Um, and
38:43 I think that's one of those cases where
38:45 it's it's yeah, it's part of my data
38:48 gathering process, but I did think about
38:50 this up front. I didn't think about how
38:52 what kind of penguins can I encounter
38:54 and do I see these in my data set? And I
38:58 didn't want to manually go through the
39:00 images because that would take a lot of
39:02 time. So I used like kind of a neat
39:03 little trick of visualizing the
39:05 embeddings to make that process easier
39:07 for myself. And then I gathered more
39:09 data by googling the type of penguin and
39:12 then the word baby after it. And that's
39:14 how I gathered more data in the right
39:16 category.
39:17 I think one of the tools that uh we can
39:20 potentially use is a tool from Vincent
39:22 which is called them better. Mhm.
39:24 Right. Just one second.
39:28 My kid needs to check that there is a
39:30 package like a postman came and he needs
39:33 to see.
39:34 Okay, now he's
39:36 Yeah, that's a that's really cool. I
39:38 actually did this project uh before I
39:40 knew about embra I would really like to
39:43 try it out again whether that makes my
39:45 life a bit easier.
39:46 Yeah, for anyone who is watching this
39:47 right now or listening to this uh there
39:49 is a video from Vincent in our channel.
39:52 It was published I think this week. Um
39:56 it's called open source spotlight embed
39:58 and bulk. So Vincent wrote two tools.
40:01 Yeah, that's a I'm really amazed by how
40:04 he like turning his ideas into these
40:07 small little projects and then just
40:09 publishes them in open source. So yeah,
40:12 so the approach for you is you need to
40:15 think about all the situations where
40:18 it's possible to encounter a penguin or
40:20 like if we generalize uh all the
40:23 context, all the situations where we can
40:26 see like the objects we're detecting,
40:28 we're understanding and then see if we
40:31 miss anything. Like for example, if I go
40:34 back to this data set with cups and
40:37 glasses,
40:38 perhaps I need to think about the
40:40 conditions like the light conditions for
40:42 example. Okay, I need to have images
40:45 that are uh well lit when it's dark,
40:48 when it's bright, right? And then
40:50 different angles like sometimes maybe in
40:53 some situations I see the handle of a
40:55 cup, in some situations I don't, right?
40:57 And then I need to like from different
40:59 ang cups from different angles and so
41:02 on. Then uh Okay. So it's just uh
41:07 sitting and thinking maybe taking notes,
41:10 right?
41:10 Yeah. And I think it's important that
41:12 you do this at the start of your process
41:13 when you first gather the data. But this
41:16 can also be part of your error analysis.
41:17 When you see that your model is
41:19 specifically making mistakes on cups
41:20 where you can't see the handle, that
41:22 might be a reason for you to think,
41:23 okay, maybe I need to gather a bit more
41:25 of that data
41:26 and verify that hypothesis. So that's a
41:28 hypothesis that you can make. Why is my
41:30 model having trouble with this
41:32 particular image? Verify that with your
41:35 initial data source. See if you can
41:37 gather more data and then have a new
41:40 version of your data set and try let's
41:41 say exactly the same model again and see
41:44 if it works better now.
41:45 Mhm. And when do we stop? Like how do we
41:48 know if it's good enough?
41:50 I think that's always a very difficult
41:51 question in data science. It
41:53 depends, right?
41:54 It depends when your results are good
41:57 enough for your use case while you're
41:59 doing this.
42:00 Okay.
42:00 And how much time you have. It also
42:02 matters. For instance, it was for me
42:03 when I was doing the penguin project
42:05 because I was just sourcing images
42:07 through Google. It was very easy to just
42:10 Google um baby penguins, Adly Penguins.
42:13 That was very easy to gather more data.
42:14 But if you have to actually go go out
42:16 bit to your kitchen and photograph a
42:18 bunch of additional pictures of all your
42:20 cups
42:21 that does make it a bit more
42:22 complicated. So it's also um when is it
42:26 good enough if given the time that you
42:28 have and given the project that you have
42:30 your results are satisfactory and that
42:31 can be because of model tuning but that
42:33 can also be because of data tuning. Mhm.
42:35 And then I guess talk to subject matter
42:37 experts, stakeholders and ask them what
42:40 they think like is this I don't know 80%
42:42 accuracy is satisfiable or they need
42:45 more.
42:47 Yeah. Though I in my experience that's
42:49 also very always a very difficult uh
42:51 conversation to have because when you
42:52 just talk about metrics lots of people
42:54 just
42:55 it's just a random number. 80% sounds
42:57 nice. Highest number is best. I think
42:59 it's always so that's always a very
43:01 difficult conversation in my or my
43:04 experience to have. Uh but you can give
43:06 examples of your data as well like these
43:08 are the these are the data points that
43:09 it classifies correctly and these are
43:11 the ones that it still has some troubles
43:12 with. Is that okay for you?
43:14 Okay.
43:15 So I think you've been advocating for
43:17 this this entire interview. Don't focus
43:20 on metrics, right? Focus on data.
43:23 Yeah, I guess so. Yes. So it means like
43:25 okay like if I know that my model is
43:28 making mistakes when um it's dark like
43:31 there is no not enough light I can just
43:33 talk to my stakeholders and show okay
43:36 there's a picture of a fork but uh the
43:38 lights are turned off that's why the
43:40 model thinks it's a I don't know a glass
43:43 right are you okay with this or we need
43:46 to collect enough pictures of forks in
43:48 complete darkness then the model will be
43:51 better
43:53 okay this this is the approach you would
43:55 take. Okay. Cool. Interesting.
43:57 I think that's that's also because of my
43:59 background in the in the medical field
44:00 where we were doing deep learning but
44:03 explanability was very important because
44:05 trust in the system was a very important
44:07 thing and you don't gain trust by just
44:09 showing a graph or showing a metric.
44:13 And then another thing it occurred to me
44:14 while we were talking is that we can
44:17 take this a simple model and if our
44:20 conditions are low maybe for medical
44:22 field it's not good but if it's a simple
44:24 I don't know think classification we can
44:27 just deploy our model and see how users
44:30 play with this and collect feedback from
44:32 the users right so let's say if we want
44:34 to deploy
44:36 yeah there was a project I uh I did a
44:39 couple of years ago it was about
44:41 classifying garbage types
44:44 like u in Europe in many countries like
44:47 garbage of certain type needs to go to a
44:49 bin of certain type right uh like you
44:52 put plastic in a in Germany you put
44:54 plastic in a yellow bin and uh you put
44:57 paper in a blue bin I don't know how it
44:59 is in the Netherlands probably something
45:01 similar
45:01 yeah I I heard the municipality of
45:03 Amsterdam is doing a very similar
45:05 project where they uh they send around
45:07 cars to to notice the garbage on the
45:08 street and notify the right people to to
45:11 pick the pick that up.
45:12 Yeah. So, and then you have this model
45:15 and then you can I don't know just
45:17 deploy create an app and then see what
45:19 kind of things users send
45:22 and see if there are any mistakes there.
45:24 Like for example, if uh the model says
45:26 that paper should go to um like the
45:30 black bin which is for everything else
45:32 that doesn't fit the other bins like
45:34 maybe you can understand okay you can
45:35 try to understand why the model is
45:36 making these mistakes. What kind of
45:38 things are we missing?
45:41 And is it because like our data set is
45:43 wrong or because our model is not so
45:45 good? How can we fix this problem? And
45:47 then probably the reason for that is uh
45:50 data right and then we
45:52 very often it is
45:53 very often it is
45:53 very often it is
45:55 do you think it's
45:55 I think it's important
45:57 sorry
45:58 yeah I think it's important I think an
45:59 important part of datacentric AI is
46:01 focusing on the data. It's also about
46:02 how do you how do you actually label
46:04 your data? How do you know that it's
46:05 that it's good? Exactly what you just
46:07 said. collecting user feedback can be a
46:09 very very good way to get more knowledge
46:11 about the data that you have.
46:15 Is it a typical approach how we put this
46:18 in production or
46:21 like maybe there are other approaches we
46:24 just roll it out and see how users react
46:26 because I I guess if we talk about
46:27 medical field we can just we cannot just
46:30 deploy this model to um I don't know
46:33 lung cancer uh things right and then let
46:37 let people just use it and uh you know
46:40 correct data later
46:41 that's simply not applicable there we
46:44 need to use a different approach while
46:45 in case of garbage classification it's
46:47 okay if uh one piece of paper will end
46:49 up in the wrong bin
46:52 yeah so that's actually interesting
46:53 because um we did do this with the the
46:55 medical imaging software we actually did
46:57 deploy it in hospitals and uh but in a
47:00 of course not very broadly at first with
47:03 a few people who were interested and
47:05 they volunteered these radiologist
47:07 volunteered to have our software run
47:09 next to their day-to-day job. So, they
47:12 were still unresponsible for their for
47:14 judging the scans. The software was not
47:17 making any decisions. I think that's a
47:18 very important thing when you're still
47:19 developing, but they did see the model
47:23 results of the software and we talked to
47:25 them extensively about the feedback. So
47:27 we got a lot of feedback for instance
47:29 that certain mistakes were being made or
47:32 certain types of mistakes were being
47:33 made and that at that point already led
47:36 us to gather more data of those kinds of
47:40 uh those kinds of examples.
47:43 I think it's called shadow mode
47:46 deployment or something like that. So
47:47 you deploy this thing in addition to
47:49 whatever process is there and then you
47:52 just use this to collect data and then
47:54 you compare whatever decision the model
47:56 is making with the decision of the
47:58 subject matter expert in this case a
48:00 doctor right and then you see the the
48:02 where the model is wrong and where it's
48:05 right
48:05 yeah exactly
48:06 I think we used something like that for
48:08 moderation so where I work at Elix it's
48:11 online classified this like a place for
48:14 selling secondhand stuff Um I think in
48:17 uh the Netherlands you have mark plots.
48:20 Yes.
48:21 So similar to that. Um and then in one
48:25 of the projects we just let it run in
48:28 parallel to moderators and then we
48:30 compared the output of moderators with
48:32 the output of the model and we concluded
48:34 that the model is good enough or like
48:37 there were of course some issues but
48:39 then after one or two iterations we
48:41 concluded it's good enough. Yeah.
48:43 Interesting. I was asking you about that
48:44 like how do you know if it's good
48:46 enough? But for us it was talking to
48:48 these moderators and thinking okay like
48:49 do you think it will help you or not?
48:51 Yeah.
48:52 I think that's the question that you
48:53 need to answer eventually.
48:55 Do you think this is a help?
48:56 Mhm. And after that we just rolled it
49:00 out.
49:02 I guess it summarizes pretty well what
49:04 we've been discussing so far, right?
49:06 Yeah, I think so do.
49:09 What if we have a lot of bait data? Like
49:11 what do we do? And um it's maybe not so
49:14 easy to collect new data.
49:19 That's just a very difficult situation
49:21 if you have a lot of bad data and you
49:23 can't collect new good data and you
49:25 can't relabel the data. I guess there
49:27 you could do a lot of manual work to
49:29 make your bad data a little bit less
49:31 bad. But
49:33 it's the same if you don't have enough
49:35 data then some some problems just aren't
49:37 solvable. I was talking at some point to
49:39 someone who offered me a project who
49:41 said, "Uh, yeah, also I I want you to
49:44 classify 13 different classes, but I
49:46 have 54 examples and also it's in 3D.
49:50 It's 3D images." That's not that's just
49:52 not going to work. Um, so unfortunately
49:55 there's not a clear-cut answer there.
49:56 Um, if you have a lot of bad data, it
49:58 might require a lot of manual work to
50:00 make it good data, but maybe if you
50:02 don't have enough, it's still it's
50:03 unfortunately not a feasible project.
50:06 Mhm. Okay. So you might even need to
50:10 open your favorite image editor and edit
50:13 some of the data, right?
50:15 Um maybe, but I wouldn't be very
50:17 enthusiastic about the project if that's
50:19 what I ended up doing. Okay, see
50:22 I I started I I studied artificial
50:24 intelligence because I find this hill
50:25 this whole topic this whole field very
50:27 interesting and I do try to automate
50:29 those things away because if I end up
50:31 just doing data cleaning by opening
50:33 image editors and and removing stuff
50:35 there I don't think that's the that's
50:37 the reason I got into this job in the
50:39 first place.
50:40 So maybe it's better to have a model
50:42 that is doing the editing, right?
50:44 That would be really nice. Yes. Or tools
50:46 that can help you out uh to automate a
50:48 lot of this stuff away. But then if it's
50:50 just 50 images then maybe you cannot
50:52 really do this.
50:54 Maybe not.
50:56 Okay. Another topic I wanted to talk to
50:58 you about was your role with PI data. So
51:01 I know that outside of your work you're
51:03 quite involved in the PI data community.
51:06 So you are a co-chair the co-chair of PI
51:09 data Amsterdam
51:11 and um yeah in general you're quite
51:13 active I as I said at the beginning
51:17 I think I came across your talk in PI
51:20 data Berlin this year and this is how we
51:23 decided to reach out to you was it this
51:25 year I think I think I found you there
51:28 I I spoke at Pa Berlin twice and I think
51:31 I yeah I think I did a tutorial this
51:32 year
51:33 yeah was uh What was the tutorial about?
51:38 It was about it was called serious time
51:39 for time series. It's time to take time
51:41 series seriously.
51:43 Okay, that's I do remember seeing
51:46 quite a mouthful.
51:47 Yeah. So, what what's your role there?
51:49 What do you do in the P data community
51:51 apart from giving tutorials and talks?
51:54 Yes. Um so I joined uh PI data Amsterdam
51:57 in 2019 I believe. Um and my role here
52:02 is I I basically joined it because I
52:03 enjoyed going to meetups. So I I studied
52:05 artificial intelligence. I was focused
52:07 on this very specific topic, deep
52:09 learning for medical imaging and I'm
52:11 just interested in everything about the
52:12 field. So I really enjoyed going to
52:14 meetups and learning more about
52:15 experiences of others in the field as
52:17 well. And by joining the committee I was
52:19 able to organize this as well. We
52:20 organize a monthly meetup and we
52:22 organize a yearly conference. Um so I've
52:26 organized a conference for PI data
52:27 Amsterdam. I've organized an online PI
52:29 data festival for PI data Amsterdam and
52:32 I've organized PI data global last year.
52:34 Um, and this year we're full on back on
52:37 organizing PI data Amsterdam and making
52:39 sure that we can uh create a really cool
52:42 conference that brings together users
52:44 and developers of basically open-source
52:46 packages in the data science ecosystem.
52:50 Uh, a confusion that a lot of people
52:51 have about PI data, I know that the name
52:53 is maybe a bit confusing. It's it's not
52:55 just about Py Python. It's also for
52:57 Julia and our users.
53:00 Yeah, it is confusing. I must I must
53:01 admit. Maybe it started as a Python
53:05 conference, right?
53:06 Yeah, I think so too.
53:08 You said that in 2019 you joined the
53:11 committee, but I don't think it happened
53:14 like you one day you woke up and you
53:17 then walked in the committee and said,
53:18 "Okay, do you mind if I join you?" It
53:21 was something else, right? like how did
53:23 how did it look like how did it happen
53:25 that you joined them? Um I think it was
53:28 actually 2018 come to think of it but uh
53:30 basically I was attending a couple of
53:32 meetups and at some point uh the the
53:34 committee is on stage and they said it
53:36 was actually Vincent who said on stage
53:38 hey is there anyone who would like to
53:40 join the committee in the break come
53:41 talk to me and that's uh I like
53:43 organizing these kinds of things uh
53:45 because I like to when you organize it
53:47 you have the luxury of also determining
53:49 what you organize so I get to organize
53:51 the meetups that I like to attend and
53:53 the conference that I'd like to attend.
53:55 So I uh I decided to volunteer
53:58 and that's also how we got our entire
54:00 new committee. Uh we have about 16
54:02 people in our committee at the moment to
54:04 organize the conference and all of them
54:05 joined basically because we uh we just
54:07 did a shout out at a meetup like do you
54:09 enjoy this kind of thing? Do you want to
54:10 make do you want to help shape this?
54:11 Come join us.
54:13 Mhm. Yeah. I guess it's pretty useful to
54:15 have a community and then you just say
54:17 okay like does anyone want to help us?
54:19 and there are a few people who want
54:21 and then so this whole process didn't
54:24 take it didn't sound like it took a lot
54:26 of time for you to actually from the
54:28 moment you started attending meetups to
54:30 the moment you joined the committee.
54:33 No, no, that wasn't a a long time. I
54:36 think I don't remember when I first
54:38 started joining meetups, but uh
54:40 Okay. Well, I guess when you live in
54:42 Amsterdam that there are so many meetups
54:44 and uh communities and just easy to be a
54:48 part of one.
54:49 I think that's one of the reasons why I
54:51 really enjoyed helping out with Bata
54:52 Global as well because I realized I'm
54:54 very privileged. I live in Amsterdam.
54:55 One of the reasons why I attended a lot
54:56 of meetups was because they were simply
54:58 very close to my house. So, if I didn't
55:00 enjoy it, I could just go home. Um, and
55:04 that's really nice about the city that
55:05 I'm in. But of course, a lot of people
55:06 live in places where it's not as easy to
55:09 attend meetups and therefore share that
55:11 knowledge and gather that knowledge. So
55:12 that's what I really like about Pa
55:14 Global. I personally do enjoy inerson
55:17 meetups and conferences more, but I do
55:18 think it's very important to make all of
55:20 this information as accessible as
55:22 possible. And that's the idea behind
55:24 global that it's it's for everyone all
55:26 over the world. Anyone can join. Um, and
55:29 that's why I liked helping out there. I
55:31 guess it started as uh in the in
55:34 response to the pandemic, right? So
55:36 people couldn't just go to inerson
55:39 meetups but then uh in addition to being
55:42 able to connect during pandemic, it also
55:44 allowed people from any part of the
55:46 world to join and also take like if
55:48 somebody does not live in Berlin or
55:50 Amsterdam or New York or any other big
55:53 tech hub, then they can just connect to
55:56 PI data global from their village and
55:58 take part in this. Right.
56:00 Exactly.
56:01 Cool. And what's the difference between
56:03 PI data and Python?
56:05 Yeah. Um, so I think the major
56:07 difference is that Python is for
56:09 everything Python and and PI data
56:11 besides also Julian R. I'm not sure how
56:14 Python feels about that, but we our
56:16 focus is more on the data side of it. So
56:21 um but data is actually the educational
56:24 flag of Num Focus and all the proceeds
56:26 of the conference that we organize go to
56:28 support the open source ecosystem but
56:30 specifically the packages that I as a
56:32 data scientist use a lot like numpy mod
56:36 panda scikitlearn those kinds of
56:38 packages and that's also what you'll see
56:40 that most of the talks are about whereas
56:42 pyon I would say is generally a little
56:44 bit broader than just data science and
56:47 data anal analytics
56:49 I still there's
56:51 I still think there is some bias towards
56:54 Python tools and PI data. I don't
56:57 Yes. Um I
56:58 historical reasons.
57:01 Yeah. We have to make a conscious effort
57:02 to make sure that the the R and Julia
57:04 folks are feel included as well. I am
57:07 mostly a Python user. I I started with
57:10 Java mod Java and MLOp and then switched
57:13 to Python. I've never only played around
57:15 with Julian R. But I think it's it most
57:18 of these things most of these talks
57:20 aren't really about the tools or about
57:22 the language or about the code. It's
57:24 more about the concept. So I think that
57:25 translates well into other languages as
57:27 well.
57:28 Yeah, I remember one of the talks in PI
57:30 data Berlin this year um was like it was
57:35 a general approach from uh a company who
57:38 is selling cars similar to what we at to
57:40 Elix do. That's why it was very
57:41 interesting for me to um check what
57:43 competitors competitors are doing and
57:46 they use Java for example
57:48 which is not any of these three
57:50 languages but yet the talk was quite
57:53 nice.
57:54 Um maybe you should you should rename it
57:56 to something like you know like how
57:58 IPython notebooks got renamed to
58:00 Jupiter.
58:01 Mhm. Julia R and Python that's what
58:03 Jupiter is for. Yeah.
58:05 Yeah. So maybe it should be Jupyter data
58:07 instead of PI data.
58:08 Yeah maybe. But I don't I'm very
58:10 involved with the Amsterdam chapter, but
58:11 I think there's like more than a 100
58:14 chapters. I don't think I have the uh
58:16 the authority to change all of their
58:17 those names.
58:18 Yeah, probably not going to happen,
58:20 right?
58:20 Maybe not. No.
58:22 Okay. Well, um I think one thing that
58:26 one last thing I wanted to ask you is
58:29 how can people find you if they have any
58:31 questions?
58:33 Oh, yeah. That's a good I I that's a
58:34 good question. I always really like it
58:36 when people reach out. So I'm reachable
58:38 through uh LinkedIn. I have my website
58:40 which is marishia.nl.
58:42 So just my first name for the
58:44 Netherlands and my email address is on
58:46 there as well. So feel absolutely free
58:48 to reach out. Also if you want to get
58:50 involved with my data maybe speak or
58:52 maybe get some tips there at or talk
58:55 about dataentric AI be really happy to
58:57 talk about those topics.
58:59 Okay thanks a lot for joining us today.
59:01 Thanks everyone for attending too. And
59:03 uh yeah, it's Friday today. So have a
59:06 great rest of the week and have a nice
59:08 weekend.