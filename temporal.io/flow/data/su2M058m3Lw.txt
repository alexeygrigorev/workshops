The Unwritten Rules for Success in Machine Learning

0:00 Hi everyone. I'll skip the usual
0:01 introduction. Yeah. So like just check
0:04 make sure to check the links in the
0:06 description and subscribe to our YouTube
0:07 channel. Very important and very
0:09 important.
0:11 And uh today this week we'll talk about
0:14 the unwritten rules for success in
0:16 machine learning and many other things.
0:19 And we have a special guest today Jack.
0:21 Jack transitioned from software
0:23 engineering to data science and he
0:25 worked in both as both individual
0:27 individual contributor and in leadership
0:30 roles. So he managed at some point teams
0:32 to up to 15 people and currently he is
0:36 the VP of data science and machine
0:38 learning and soon and there's even a
0:41 date you gave us a date November 15th he
0:45 plans to move to become an entrepreneur.
0:49 So yeah, welcome to our podcast.
0:53 Yeah, thanks Alexi. Thanks for having
0:54 me. Great to be here.
0:56 Uh the questions for today's interview
0:58 are as always prepared by Johanna Byron.
1:01 Thanks Johanna for your help. And let's
1:03 start. So before we go into our main
1:06 topic of these unreasonable rules, let's
1:08 start with your background. Can you tell
1:10 us about your career journey so far?
1:12 Yeah. So, I started out um my
1:15 professional career in 2015
1:18 as a full stack software developer. Um I
1:22 was working at a company called uh Trunk
1:24 Club. Um they're sort of like Stitch
1:25 Fix. They're owned by Nordstrom online
1:27 retail and uh started out as a software
1:31 developer. uh worked there for a couple
1:34 years in that role but then became
1:36 interested in data science and machine
1:38 learning and made a transition within
1:41 the company to a data scientist.
1:42 Do you remember what made you interested
1:44 in data science? Yeah. So actually I had
1:48 I had sort of always been interested in
1:50 data science back going back to my
1:52 undergrad days when I was a physics
1:53 major and took a lot of stats classes
1:56 but you know this was 10 years ago. So
1:58 at the time data science was not well
2:00 established and there was no master's
2:03 degree there was no bachelor's degree.
2:04 There was no clear career path. So I
2:08 decided to do software engineering, a
2:10 masterers in software engineering, um
2:12 intending to go into data science, but I
2:14 ended up just really enjoying software
2:16 engineering. So I stuck with it for a
2:17 couple years and then um at some point
2:20 though uh I started to become interested
2:23 in machine learning again and uh a data
2:27 science team was spun up at my company.
2:30 You know, I watched what they were doing
2:31 and became very uh interested in it. And
2:33 so uh I made the transition into that
2:36 team uh through an apprentichip.
2:39 Opportunity.
2:41 Yeah, it was not easy. Um it was you
2:43 know I had to I had to be very assertive
2:45 and ask a lot of times uh and then
2:48 eventually landed a informal
2:51 apprenticeship opportunity within that
2:53 where I was like doing a side project
2:56 but um was able to switch. did data
2:59 science at trunk club for a year and
3:02 then moved on to a company called go
3:03 health which is sort of like orbits or
3:05 kayak but for health insurance
3:08 and I was a senior machine there
3:11 orbits I think wait I think like I was
3:13 in the states at some point and I needed
3:15 to buy a ticket uh airplane ticket and
3:18 this is what you use for comparing
3:19 prices right
3:20 exactly
3:21 yeah okay it's like sky scanner
3:24 yes so think about that for health
3:26 insurance you know At least in the
3:28 United States, most of us get health
3:30 insurance through our employer. Um, and
3:32 you have like three plans to choose
3:33 from. But for people who are not
3:35 employed or people who are uh, you know,
3:38 self-employed,
3:39 uh, to choose a health insurance plan,
3:41 you know, it's it's complicated. So,
3:44 this was this platform was basically
3:45 like the the selection process for that.
3:48 Um, in any case, um, so was a senior
3:51 machine learning engineer there, started
3:52 out in that role. This the data team was
3:54 also new at Go Health and quickly I
3:58 found myself informally managing that
4:01 team. Um because you know there was a
4:03 data science team within Go Health. Uh
4:06 and the data organization itself was
4:09 maybe 15 to 20 people all being managed
4:11 by one person and so naturally you know
4:15 people stepped up as informal managers
4:17 and that was me for the data scientists
4:20 and the data analysts around me. Uh so I
4:22 was formally promoted to manager after a
4:25 year or two and then um again uh I was
4:28 promoted to director after another year
4:29 or two after that. Most of that was
4:31 around having successful projects
4:34 launched. So after a big project launch
4:36 you know that would generate a bunch of
4:38 excitement and demonstrate value to the
4:40 company. Uh leadership would want to
4:42 invest in more data science and machine
4:44 learning and naturally that would come
4:45 to me. Um I'm just curious in retrospect
4:49 do you think it was a bit
4:51 too fast too quick cuz like you worked
4:54 for a few years as a individual
4:56 contributor and then like boom you
4:58 become became a manager like was it like
5:02 you
5:03 like what like now when you look back
5:06 was it too quick or was it like just the
5:08 right pace?
5:10 It's a good question. I'll I'll give you
5:13 two short answers to that. The first one
5:15 I'll say is that um I had uh because I
5:20 did a lateral move between software
5:21 engineering and data science and machine
5:23 learning. There's so much overlap
5:25 between those skill sets and like
5:27 professional maturity that I would say
5:28 so it helped a lot. Right.
5:30 It did. Yeah. So it wasn't like I had
5:32 just one year of experience before
5:33 management. It was more like I had like
5:34 four uh before being promoted because
5:37 there's again there's just so much
5:38 overlap between the two. So I would say
5:41 that the other part though I would say
5:43 is that being promoted to manager it's
5:45 almost always like it feels too soon and
5:48 it will always feel too soon because
5:50 there just is a shift that is very
5:52 difficult to emulate a seamless
5:54 transition like it's a new paradigm.
5:57 It's a new thing to experience and it is
6:00 always going to be a difficult
6:01 transition. That being said sure it was
6:03 it was fairly fast and you know both
6:06 good and bad uh results came from that.
6:08 Mhm. But [clears throat] you also
6:09 learned a lot, I assume. Right.
6:11 Oh, yeah.
6:11 Like when you fast, you also learn fast
6:14 like because like you have no choice.
6:17 Exactly. Yeah. So that was, you know,
6:19 when I think about like the pivotal
6:20 years of my professional career, there
6:23 were two sort of time periods. The first
6:25 was just my first couple years at Trump
6:26 Club being in part of a strong
6:28 technology organization. Develop a lot
6:30 of really good fundamentals and you sort
6:32 of like get into your groove. And then
6:34 the the next phase was going from a
6:37 senior IC role to director within a
6:39 couple years. That was just you know
6:41 very very different from the other part
6:44 but um learned a lot you know that's
6:46 where you learn a lot of things that
6:48 like are not really taught anywhere.
6:50 It's very difficult to teach them
6:51 because
6:52 everything's nuance piece of data
6:55 science right so you just have to
6:57 like yeah how do you actually learn the
6:59 skills?
7:00 Yeah. Uh I mean for me it was trial and
7:02 error. It was it was realizing what
7:04 worked and what didn't work. But um yeah
7:07 how do you even know like
7:09 yeah a lot of that is what I'll is what
7:11 I'll share you know in general is just
7:14 like what works what doesn't work why
7:15 does it work and you know in fact that's
7:18 a lot of the reasons why I started to be
7:20 more proactive on social media. Um but
7:23 yeah that was a great learning period
7:24 for me. Um, you know, you learn a lot of
7:27 lessons around
7:30 being able to convince people uh of
7:32 value, you know, being knowing when and
7:35 how to articulate accuracy versus versus
7:38 something that may be less precise but
7:40 more compelling, things like that. You
7:43 know a lot of being a leader is you have
7:45 to in some sense be a salesperson uh to
7:47 the rest of the business where you have
7:49 to be able to demonstrate value. You
7:51 have to sell value. You have to
7:53 translate you know why is this needed?
7:56 You know trying to convince a
7:57 non-technical stakeholder why you have
7:59 to spend a month on cleaning up tech
8:01 debt. You you need some sales skills to
8:03 do that. Well, um, so there's just a lot
8:05 of things that are very, you know, it's
8:07 difficult to teach those in in any kind
8:09 of technical curriculum because they're
8:11 so opposite of what needs to be
8:14 emphasized early on in that career.
8:16 Do do you have engineering background,
8:18 engineering education, like software
8:20 engineering?
8:21 Yeah. So my my master's degree was in
8:25 software engineering. Um, and then my
8:27 first couple years were software
8:28 engineer. So I assume it's not uh a
8:31 skill you picked up during studies,
8:33 right? So like during software
8:34 engineering classes,
8:36 you are not taught like how to sell
8:38 things. You you learn how to I don't
8:41 know do Java and algorithms and all that
8:43 stuff, databases, right? So you're not
8:46 uh you're not learning how to sell
8:48 things. So how did like it's it was all
8:50 trial and error, right? So you just see
8:52 like how you approach a person and you
8:54 try to kind of sell a project or
8:57 something and then you see you feel what
8:59 works, what doesn't, right?
9:01 Yeah. I would say it's a mix of trial
9:03 and error and it's also a mix of
9:05 observing works for others. So um you
9:09 know another thing that is not really
9:11 taught very often in any kind of school
9:14 environment is the importance of
9:17 technical problem framing and
9:19 understanding the business and user side
9:22 of the applications you're building. Uh,
9:26 one one uh, story I like to tell is when
9:29 when I was a young software developer,
9:32 you know, a couple years into my career,
9:34 I felt like I was really strong
9:36 technically. And then I would look
9:38 around myself at like the people on my
9:39 team and think, okay, who's better at
9:41 what? How do I make myself better? And
9:44 then I would look to my tech lead and
9:45 see, okay, what makes them special? And
9:48 something I observed was that my tech
9:49 lead at the time was strong technically,
9:53 definitely more strong than or stronger
9:54 than I was because they were more
9:56 experienced. But relative to the people
9:59 around him, he was not necessarily that
10:02 much stronger. But there was a clear
10:04 difference in how much he could produce
10:06 and the value that he provided. And the
10:08 difference was that he was really really
10:11 good at taking the time to understand
10:14 deeply the applications that we were
10:16 building and you know taking our
10:19 two-hour meetings with stakeholders to
10:21 ask them questions and really just
10:23 understand the aspects of of the product
10:27 as well as anyone else did at the
10:28 business. Because once he understood
10:30 that he could then come back to the tech
10:32 team and be able to transit. Okay,
10:34 here's what we need to do. Here's what
10:35 we need to prioritize. here's why this
10:37 is the way that it is. And that is
10:41 something that's very difficult to
10:42 teach. You know, it's difficult to teach
10:44 because when you're learning software
10:46 development, you're so overwhelmed by
10:48 everything else that hearing someone try
10:51 to explain that is like, well, that's
10:52 obvious. You know, I can focus about
10:54 that later. It's very difficult to sort
10:56 of like mentally allocate the right
10:58 capacity to something like that when
11:00 you're getting crushed by all of this
11:01 very difficult technical stuff. So
11:03 that's sort of something that you know
11:05 even on LinkedIn when I post if I post
11:07 something about that the amount of
11:09 interest that I'll get relative to
11:11 posting something very technical is so
11:13 much lower right because most people are
11:15 not don't feel like they're struggling
11:16 with that right now they feel like
11:17 they're struggling with the technical
11:18 parts and so that is the more valuable
11:20 piece to them and so this these other
11:22 parts that really do help differentiate
11:24 people in their careers often go
11:27 unnoticed and are not learned until
11:29 trial and error several years
11:33 So like one of these things that go
11:35 unnoticed as you mentioned is u
11:39 understanding
11:41 the applications right on the deep level
11:44 and understanding what um I know what
11:48 generates value right that's one of the
11:49 things as you mentioned this tech lead
11:52 had that other maybe less less senior
11:55 people did not are there other things
11:58 like these hidden things that uh are not
12:01 obvious when you study when you like
12:05 focus on technical things but that are
12:07 actually very important at work.
12:10 Yeah. So,
12:13 one thing that's extremely important is
12:15 people's perception of you,
12:18 another thing that is never taught in
12:20 any course that I've ever seen is in
12:23 order to be successful in any especially
12:26 in leadership, but really in any role,
12:28 people have to respect your opinion. And
12:30 you know, when you ask for things, they
12:32 have give them to you. Like that is just
12:34 an essential part of being a high impact
12:37 uh either IC or manager. And in order to
12:40 do that well, you people have to respect
12:42 your opinion. And developing a
12:45 reputation for respect and, you know,
12:48 strong consideration is something that
12:51 is difficult and uh is something that I
12:53 learned just over time how to do. You
12:55 know, one simple tip that I give to
12:58 people is anytime you're given an
13:01 opportunity to do a presentation in
13:03 front of an audience,
13:05 spend way way more time on that than you
13:08 think you should. Like anytime you are,
13:10 you have an opportunity to speak to
13:12 people that you don't normally speak to.
13:14 they will they will base their
13:16 perspective of you very heavily on that
13:20 discussion or that presentation because
13:22 that's really their only perspective
13:24 into you is that's their only
13:25 interaction with you. And so, you know,
13:28 if you're giving a presentation to
13:29 leadership or even just other teams, if
13:32 those teams aren't working with you
13:33 directly frequently, they don't know how
13:35 good you are. They don't know any of
13:36 that. And so you really have to make
13:38 sure that anytime you're given an
13:40 opportunity to interact with others who
13:44 uh you don't interact with on a
13:46 day-to-day basis, you have to take
13:48 advantage of that. And that's where like
13:50 the sales stuff comes into play where
13:52 you know maybe maybe saying things as um
13:57 a as they are is not always the best
13:58 thing. You want to sell them, you want
14:00 to promote them. And really what you
14:02 should be thinking about is like what is
14:03 the ultimate action that I want to be
14:05 taking? So, you know, if you're thinking
14:07 about the the month of stakeholder or
14:10 I'm sorry, the month of technical debt
14:12 cleanup, you know, if you try and
14:13 explain the exact reason why that's so
14:15 important, you probably won't get what
14:17 you need. And so, you need to be able to
14:18 communicate it in another way. And you
14:20 need to be able to uh you need to be
14:22 able to put on a persona that is well
14:24 respected so that you get what you want.
14:27 So, that that's another aspect that is
14:28 just like, you know, it's more of a
14:30 quote unquote softer skill,
14:32 but it's critical to success. You know,
14:34 a lot some people can get to the top
14:36 through just technical skills, but
14:38 they're very few and far between. The
14:39 people most people who get to the top
14:42 are doing so through non-technical
14:43 means.
14:44 So, look for opportunities to speak in
14:47 front of people who you don't interact
14:49 with regularly. And when you get this
14:52 opportunity, invest a lot of time in
14:53 preparing the presentation that you
14:56 give. Right.
14:57 Yes. And probably like you should avoid
14:59 technical things like in your example of
15:01 technical debt like you shouldn't say
15:05 like I don't know we need to refactor a
15:07 lot of code cuz like our classes are too
15:09 long right like you should probably come
15:13 up with a good metaphor right instead of
15:15 explaining like the code like you
15:17 probably
15:18 find a relatable
15:21 uh idea from the real world and then
15:23 explain based on that right
15:25 yeah and uh even extending that um
15:29 something that I like to do is to think
15:30 through think through what is uh what
15:34 people care about you know so if you're
15:36 working professionally and you have
15:37 let's say you're interfacing with like
15:39 marketing marketing will obsess over
15:42 things like tech cost per acquisition
15:44 you know all they care about is
15:46 conversion and CAC and things like that
15:48 so if you're trying to present to them
15:49 something like you know related to
15:51 machine learning data science or
15:52 whatever it is uh if you can talk to
15:54 them in terms of you know impact act to
15:57 CAC or some or something that you know
15:59 that that they will care about, you'll
16:01 get a lot more attention.
16:03 Sorry.
16:04 Uh CAC, so CAC, cost per acquisition.
16:07 Cost per acquisition.
16:10 So it's it's the amount of money that
16:12 marketing needs to spend in order to
16:14 acquire a customer on average.
16:16 So it's like their primary uh KPI or key
16:19 metric.
16:20 So like when you speak with the
16:22 marketing people, you need to learn
16:26 their vocabulary, right? So you need to
16:28 say like to to to
16:30 to use words like this um cost per
16:33 acquisition and so on, right? Cost per
16:35 customer. I already forgot. And uh so
16:38 then they can already relate to what
16:41 you're talking about. Right.
16:43 Right. And and once [clears throat] you
16:44 start using that terminology or
16:46 vocabulary, now they feel like you
16:48 understand them and will start to treat
16:50 and will start to have more respect for
16:52 you because what often happens is, you
16:54 know, technology people speak
16:56 technology, marketing people speak
16:57 marketing and both sides feel like the
17:00 other doesn't understand what they think
17:02 is important and therefore does not
17:03 understand anything important. And so
17:06 you kind of talk past each other a lot.
17:08 But if you're able to speak at their
17:10 language, you're able to uh get their
17:13 respect and now they'll actually start
17:14 to listen to you more seriously because
17:17 they feel like you know what matters to
17:20 them.
17:21 Mhm. Okay. So, uh we will see how
17:25 relevant it is. I hope it is relevant to
17:27 the actual discussion we plan to have
17:29 today which is like the rules for
17:30 success in machine learning. Probably it
17:32 is related. I just want to summarize
17:34 like if you want to be like a technical
17:38 leader there are some qualities that uh
17:41 good to have like understanding the
17:44 application and then the important thing
17:46 is people's perception of you. So you
17:48 need to gain respect from them and for
17:50 that you need to appear in front of
17:52 other people and speak the language. So
17:54 use their terminology and then you kind
17:57 of become bias or like they treat you as
18:01 one of their own, right? swim and this
18:02 way you get respect. So these things
18:05 that we discussed they are pretty
18:07 interesting to discuss like how related
18:10 they are to successful machine learning
18:13 projects
18:16 uh very very uh well intertwined. Um so
18:22 especially with machine learning uh like
18:24 uh you know any kind of project but
18:26 specifically machine learning one of the
18:28 challenges with machine learning is just
18:30 how complex it is and how many how much
18:33 support you need from every part of the
18:35 business for machine learning to work.
18:37 You know any software developer will
18:39 tell you like getting support for a new
18:41 project is difficult and that's
18:43 absolutely true. But the level of
18:45 support that you need to execute a
18:48 machine learning project that actually
18:50 works and provides value is, you know,
18:53 on I I'm going to pull out a number
18:54 here, but maybe 3x what a typical
18:56 software project does. And that's
18:58 because there's so much level of care
19:00 that needs to be involved with
19:02 generating data, transforming it,
19:04 setting up that pipeline, building the
19:06 model, building the prediction pipeline.
19:08 There's just so many things that can go
19:10 wrong and so much effort that needs to
19:12 be taken that in order to allocate the
19:15 resources for that you have to have a
19:18 lot of motivation and a lot of buyin
19:21 from stakeholders. So being able to sell
19:24 is a very crucial skill of pretty much
19:27 any machine learning person because in
19:29 order to gain enough support for your
19:32 projects, you have to be able to get
19:34 people excited about the potential of
19:36 what of what you're going to build. And
19:38 so that's something that I learned, you
19:40 know, maybe a year or two into my career
19:42 in machine learning was that I was a a
19:45 lot of the projects I wanted to build
19:46 were dead in the water because I
19:49 couldn't get enough support. So what I
19:51 started to do was to figure out the the
19:53 ways in order to generate excitement and
19:55 get that support that I that I needed.
19:58 One example is uh being able to uh get
20:02 to a proof of concept that I can uh that
20:05 I can use in a demo very quickly to show
20:08 on like a real sample data set like a
20:11 prediction in real time. That is very
20:13 effective at getting interest between
20:15 people because they can see like the the
20:17 potential value of it. But going back to
20:19 your question, so like how do these
20:20 skills intertwine with successful
20:22 machine learning or you know the rules
20:24 of machine learning? You have to
20:27 understand that like machine learning,
20:28 you don't just get handed machine
20:29 learning projects and your only task is
20:31 to execute on it. That does not happen.
20:33 A huge part of a huge part of your role
20:35 in machine learning is to be able to
20:37 communicate back value to by yourself,
20:40 you know, the the bandwidth and the
20:42 resources that you're going to need in
20:43 the future. That's a very often
20:45 overlooked aspect.
20:48 So you mentioned that uh these machine
20:51 learning projects are quite complex and
20:53 then they require 3x like arbitrary
20:56 number but they require a lot more
20:58 effort than a traditional usual software
21:00 engineering project. Not to mention that
21:02 these projects often fail right so
21:04 because you don't know in advance
21:06 whether the project will succeed after
21:07 you put so much effort in them like how
21:10 do you sell that like uh by doing this P
21:13 and showing it kind of sort of works or
21:17 Yeah. So, there's a couple things going
21:19 for you with machine learning that you
21:20 can leverage to your benefit. Right now,
21:23 it's actually this is the best time ever
21:25 for this because large language models
21:27 have generated excitement across the
21:28 entire planet. So, in this moment, it is
21:32 much easier to sell people on machine
21:33 learning than it was four or five years
21:35 ago.
21:35 Like there is this hype everywhere like
21:37 open any website and you see GBT, LLM,
21:40 whatever.
21:41 It it's a double-edged sword where
21:43 there's benefits to that and negatives
21:45 to that. However, even so, it is still
21:47 difficult to sell projects because like
21:49 you said, there is so much risk
21:51 involved. Uh, a lot of what people
21:54 really uh like what the connection that
21:57 people make with machine learning is
21:59 they want to automate their decision
22:02 making uh or what they think is possible
22:05 with decision- making. So, a lot of
22:07 people you can you can play off of that.
22:09 So you know for example if you wanted to
22:11 build a model that was prioritizing
22:13 incoming inbound leads for your you know
22:16 your sales process the pe there are
22:19 people at your company who are they
22:21 spend their days trying to think through
22:23 how do we ensure that we're um properly
22:26 handling like the highest uh quality
22:29 leads the people who are actually going
22:30 to buy and spend a lot of money like how
22:32 do we ensure that they're treated well.
22:34 Well, if you can build a model that you
22:36 know, if you can showcase a simple model
22:38 that says we can detect which leads are
22:41 going to be high value, that is like
22:44 very very easy for people to realize the
22:47 potential impact or use for that. And if
22:49 you can give them even just a little bit
22:51 of evidence that you're capable of
22:53 building that that then they'll buy into
22:55 that. So really, you want to like play
22:57 to what they're what they care about and
22:59 then give them some evidence of that and
23:01 showcase it. And usually like visuals
23:03 are very uh are really good selling
23:06 points here. And if you can give them
23:08 that, you're much more likely to get
23:09 that buy in. And if you have
23:11 stakeholder, you know, support, you can
23:13 typically get the engineering resources,
23:15 all that stuff
23:18 by visuals. So you said visuals are
23:20 important to them. What do you mean by
23:22 that? like is it uh is having a demo
23:25 with user interface where they can play
23:29 is important or you meant something else
23:31 or like having a picture planting a
23:32 picture in their head or something else.
23:36 Yeah. So it's a good question. Um let me
23:38 make another comparison uh using like uh
23:42 the domain from a different company I
23:44 worked for. So, I worked for Wayfair uh
23:46 a couple years ago and you know using
23:50 this context just because it's easy to
23:51 understand furniture.
23:52 They sell furniture, right?
23:53 Yeah. Yeah. So, we the model that I'm
23:55 going to describe we didn't like I
23:57 didn't build but here's a hypothetical
23:58 model and here's how two ways that you
24:00 could pitch it. Uh and one would be
24:02 effective and one would not be. So let's
24:04 say you wanted to build a uh a model
24:07 that could detect someone's um
24:11 preferences for like styles of
24:13 furniture. So let's say that it's you
24:16 know like I like rustic furniture uh
24:19 farmhouse or something like that whereas
24:21 maybe you like modern furniture or
24:22 something and I could let's say that I
24:26 built a proof of concept model and
24:27 wanted to pitch this. I could show
24:30 people, you know, the accuracy of uh
24:33 items bought that were related to the
24:37 style of furniture that I said they
24:39 were. So, you know, somebody purchases a
24:41 couch with 70% probability, I can
24:45 predict which type of style it is based
24:47 off of my model. That is not a good way
24:50 to sell your model. A better way to sell
24:52 your model is to go and show three
24:55 examples of, you know, stylistic
24:58 preferences that a customer has and then
25:00 uh that they purchased in the past and
25:02 then show them the next three items that
25:04 they might purchase in the future
25:06 because they have similar styles. So,
25:08 this is a good way to visualize
25:09 customer. You pick a customer, a random
25:12 customer and say, "Okay, this is uh the
25:14 person next and this is what they like.
25:16 This kind of furniture they like, right?
25:18 So these are the preferences and based
25:20 on that we think that the the next three
25:23 orders will be these things
25:25 right and what that does is it gives
25:28 your audience your stakeholders uh it
25:31 shows them like loosely how the model is
25:33 thinking about things. So, you know,
25:35 maybe the two different couches are the
25:37 same material or something and they can
25:39 start to mentally wrap their head
25:41 around, oh, here's what the model is,
25:43 how it's able to think, and then they
25:45 can take that and they can generalize
25:47 that to all these other things that they
25:48 care about where it would be useful to
25:50 have an automated decision-making
25:52 process that could detect that. So
25:54 really you're trying to get them to
25:55 understand the model and what it can do
25:57 and then let them take that notion and
26:00 like run using their own you know
26:02 internal knowledge communicating things
26:04 in like an accuracy where at the at the
26:06 end of the day those are the things that
26:08 matter but they're not great for selling
26:10 because that's not how your stakeholders
26:12 are thinking about things.
26:15 And if you start talking about accuracy,
26:18 right, and you say like, okay, this
26:19 model is 70% accurate, which like which
26:22 may or may not be a good number
26:24 depending on the the model, right? But
26:26 like to the stakeholders, it might sound
26:28 scary like h 30% error rate like 30% of
26:31 times it will make mistakes like oh how
26:34 bad do you handle that?
26:37 Um, yeah. So
26:39 thinking about that or
26:42 so
26:44 it's a it's a good question and uh you
26:48 know t actually of the two problems that
26:50 is the easier problem is trying to
26:51 communicate error rate and things like
26:53 that. So typically what I like to do
26:55 actually is not communicate accuracy
26:57 because it is the easiest one to pick
27:00 apart and say like this is not good
27:02 enough. What I try and do is discuss
27:04 precision and recall because like
27:07 when you discuss
27:08 they're quite technical right like it's
27:09 uh like you need to give very good
27:12 illustrations to people cuz like even
27:15 data scientists I remember studying
27:17 machine learning at university and like
27:19 I was always lost at precision recall
27:21 like for me it was super confusing like
27:22 I always confused the two like it took
27:25 me some time to actually feel
27:26 comfortable with these two metatics
27:29 yeah so I don't actually use the words
27:31 precision or recall Mhm.
27:32 But what I'm actually like what I dis
27:34 what I am discussing is precision
27:36 recall. And what I say is I'll show like
27:38 for example uh a sliding bar of how
27:42 specific do you want or how how much of
27:45 a threshold do you want to draw between
27:47 overguessing or underg guessing
27:49 and so typically I'll use precision and
27:51 recall and then discuss and it the goal
27:54 is not necessarily to
27:56 give them exact numbers but it's to
27:58 inform them that there is we can control
28:01 the trade-off that we're making and it
28:03 gives them more like they're more
28:04 comfortable with that because it means
28:06 Oh well, if we're overpredicting, we can
28:08 just, you know, take that slider back.
28:10 And they remember the slider. They
28:11 remember that there is a threshold and
28:13 it gives them more comfort to know that
28:15 they're in control.
28:16 Do you use something like streaml or
28:18 gradio for that or you use your full
28:20 stack development skills and like build
28:22 the reactor?
28:23 No, I I do whatever I can generate in
28:26 the shortest amount of time that looks
28:28 decent is what I do,
28:29 which is like usually grad or something
28:31 like that, right?
28:32 Yeah, sometimes. Um, a lot of the times
28:35 it depends depends of the organization.
28:36 You know, if I have more time, I'll do
28:38 something like that. Honestly, sometimes
28:39 I'll just go into Google Sheets and
28:42 something that's very uh very very
28:45 basic.
28:46 I see. I see.
28:47 Yeah.
28:47 Yeah. Interesting. Well, we still wanted
28:49 to talk about the rules of machine
28:51 learning and the unwritten ones. And
28:54 there is this famous article from Google
28:57 which is called the rules of machine
28:59 learning, right? You probably know about
29:01 that. And I remember like the rule
29:02 number one is don't be afraid to start
29:06 without machine learning. That's my
29:07 favorite one.
29:09 Like and it's kind of funny cuz like
29:11 you're a data scientist or machine
29:12 learning engineer like how can you
29:14 advocate for not using machine learning?
29:16 Like it's a bit counterintuitive, right?
29:18 And then like the they talk about
29:20 metrics and so on. So this is definitely
29:22 a really good article to talk about.
29:25 [snorts] But since today we talk about
29:27 the unwritten rules. So, I'm wondering
29:30 what these rules are. [sighs]
29:33 [snorts]
29:33 Yeah, awesome question. There's a lot of
29:34 them. So, I I think actually let me
29:39 touch on what you just mentioned really
29:40 quick because I think it's so like
29:42 regardless of if it's written or not
29:44 written, I think it's
29:45 maybe let me expand on that because the
29:48 importance of that which is which is
29:49 often not written. So I completely agree
29:52 with that statement that it is critical
29:54 to and I I would even go beyond saying
29:57 don't be afraid to start and I would say
29:59 always try to start without machine
30:01 learning and the reason for that is
30:03 because the most often the most frequent
30:06 reason that I see machine learning
30:08 projects fail is because whatever you
30:11 end up building doesn't actually solve
30:13 any problems. So even if you build an
30:15 accurate machine learning model that
30:17 that does not necessarily mean that
30:19 whatever it's doing is going to provide
30:21 value to the business. And so ensuring
30:24 that you are before you build machine
30:27 learning ensuring that whatever problem
30:29 you're solving is actually worth solving
30:32 with or without machine learning that is
30:35 that should always be the first focus.
30:37 So you know I always emphasize you
30:39 should do a proof of concept heristic
30:41 like rulebased model first. Forget
30:43 machine learning, forget all the
30:44 complexities with that. Try to just spin
30:47 up a a manual process or a rule-based
30:50 process that emulates what a machine
30:52 learning model is going to do, but does
30:54 so at a at a much more basic level. And
30:57 honestly, nine times out of 10, if if
30:59 you can't make that work, the machine
31:01 learning model is also not going to
31:02 work.
31:03 So about that.
31:05 Yeah. So like a few years ago,
31:09 so we wanted to launch a model for
31:11 predicting the quality of pictures like
31:14 if a picture has a good quality like it
31:17 was an on in online classified. So there
31:20 people would sell and buy cars, right?
31:23 [snorts]
31:24 So one idea was that to make listings
31:26 more attractive, we can look at listings
31:28 with bad quality pictures and if there
31:32 are bad qual if there are bad pictures,
31:34 we contact the sellers and ask them to
31:36 improve the picture, which is like
31:38 probably a reasonable assumption that if
31:40 they improve then probably more people
31:42 will become interested in that. And then
31:45 we started building the model, a deep
31:47 learning one of course cuz it's images,
31:48 right? Uh, and then like a few months
31:51 after we were finally ready to test it,
31:54 turned out that nobody really cared.
31:55 Like the thing that people mostly care
31:58 about when looking at listings is price,
32:00 right? Not quality of images. And like
32:03 the the sellers, they aren't really
32:05 super reactive to these suggestions. So
32:08 what what could have happened instead is
32:10 like I just sit down, take a sample of
32:14 images and say this image is good, this
32:16 image is bad. We sent a an email to the
32:19 settler saying, "Okay, your image is
32:22 bad. Like, do something about this and
32:24 see how many people react one day of
32:26 work, right?"
32:28 And then like, "Okay, nobody cares about
32:30 that. So, let's just, you know, put this
32:32 idea aside for some time and focus on
32:36 something else."
32:39 I love this story cuz like it's
32:40 something I experienced myself, right?
32:43 And then like one thing is somebody when
32:45 somebody says like you just not and say
32:47 yeah but like having experienced this
32:50 first hand is like totally different
32:51 thing.
32:53 I it exactly that is exactly like the
32:56 type of situation that occurs and again
32:59 it also is uh evident in software
33:03 engineering where a lot of people ask
33:05 for things you know 10 years ago or
33:07 whatever the common thing was can you
33:09 build me an app that does this that was
33:11 the question everyone wanted was I have
33:13 an idea for an app and you know after or
33:16 I need a feature that does this and
33:19 eventually you you realize that you have
33:21 to start vetting who is asking that
33:22 question why they're asking it because
33:24 oftentimes people think they want
33:25 something and then in reality they don't
33:27 even use it. So that it's basically like
33:29 the same problem as that but uh much
33:33 more intense because machine learning is
33:35 so much more difficult and uh expensive
33:39 both in terms of actual monetary costs
33:41 and uh development time that making a
33:44 mistake here is much more costly than in
33:47 just a typical software development
33:49 project usually. So it's definitely very
33:52 critical. So I just wanted I wanted to
33:53 highlight that point because you know
33:55 when you think about like the unwritten
33:56 rules of machine learning is that you
33:59 really have to understand that sort of
34:01 like the na the human nature of getting
34:04 lost in the technical details in machine
34:07 learning projects that all of us
34:09 Yeah. And then you establish a baseline.
34:12 Well, you first prove like if this idea
34:13 is valuable at all. Then you can
34:16 establish baseline and then you have you
34:18 can iterate on top of that and you can
34:21 always see like if this is actually an
34:23 improvement over the previous iteration.
34:25 Right.
34:26 Exactly. So one another concrete example
34:29 of this with machine learning was at at
34:30 one point in my career I built a
34:32 customer churn model. So you know in in
34:35 any subscription based company uh churn
34:38 is uh when they cancel their
34:40 subscription and uh for the company in
34:43 question I won't mention which one it is
34:45 uh just to keep things uh somewhat
34:47 private is uh we we tried to build a a
34:50 churn prediction model and I did and it
34:52 actually was pretty accurate. It could
34:54 detect who was going to churn. The
34:56 problem with that was there was nothing
34:58 we could do about it. And like we we
35:00 thought that there was going to be
35:01 something we could do when we were able
35:03 to identify someone who was about to
35:04 churn. But in this in this case there
35:07 was really like no there was no action
35:10 to take that was likely going to make
35:11 them not turn. And so because of that
35:13 the whole model was useless, right? Like
35:15 it didn't provide any value. And the
35:17 reason for that is because it wasn't
35:18 actionable. And had we gone through like
35:21 a more you know heristic based approach
35:24 we would have saved ourselves several
35:25 months of development time.
35:27 Was it the case cuz um when you try to
35:31 approach people and prevent them from
35:34 not journeying they are annoyed even
35:35 more or
35:38 not. So I I would guess that that is
35:41 often the case. In this case, it wasn't.
35:42 And it was more so it was because of
35:44 like the reason that they were turning
35:46 was not because they were unhappy, but
35:48 because of external factors that were
35:50 driving them to turn. Again, I don't
35:52 want to get I don't want to get too into
35:53 it because then you would give away the
35:55 the company.
35:57 Yeah.
35:58 Situation like let's say there is a
36:00 telecom provider and people are turning
36:02 because like there is a different
36:04 provider with better prices and you
36:06 cannot lower lower your prices.
36:09 Exactly. Exactly. That
36:11 lower then you kind of work uh um like
36:16 you don't generate enough revenue
36:19 right your margin is a bad example
36:21 yeah okay well so the first unwritten
36:25 rule h I'm wondering so in this podcast
36:28 we have um transcripts right so like
36:32 after this is transcribed this rule
36:34 becomes written or [laughter]
36:37 yeah I well most of this stuff I'm
36:39 already posting on LinkedIn anyway way.
36:41 So I guess at this point
36:42 too late like the commonly unwritten
36:45 rules by what Jack has written about
36:48 them, right? [laughter]
36:50 Okay. So the first rule is like we get
36:55 caught up in technical details and we
36:58 what's the expression? We lose like we
37:01 don't see the forest behind.
37:03 We lose sight of Yeah. We lose sight of
37:05 the end goal.
37:06 Yeah. So we lose sight of the end goal
37:07 and then like we just get too interested
37:11 to invest it in details like tuning this
37:13 like because it's exciting to to tune
37:16 all these knobs of a deep learning model
37:17 for example or any machine learning
37:19 model anyway. So that's one thing. So we
37:23 don't we need to focus on the end goal
37:25 and sometimes it means like doing things
37:28 manually. Right.
37:31 Okay. Exactly.
37:34 The next one. Uh so here's another
37:36 unwritten rule and that is uh well let
37:42 me let me describe the more common uh
37:44 written rule and let me describe what I
37:46 think the unwritten version should be or
37:48 what the version should be that is when
37:49 written. So the commonly
37:53 uh told rule is you should be obsessing
37:57 over data and obsessing over
37:59 understanding the patterns and the
38:01 distribution and the nuances to it. And
38:04 I agree. However, I think that that is
38:07 somewhat misleading. And I think a
38:09 better way of phrasing it is that you
38:11 need to understand not only the data,
38:14 but you need to understand the process
38:16 for the things that the data represents.
38:19 Uh that that is really what you should
38:21 be obsessing over. So for example, um if
38:25 you are looking at customer buying
38:27 patterns for an online retail company,
38:31 you need to understand who your audience
38:33 is. You need to understand how they like
38:35 to buy. You you it's even better if you
38:37 yourself are a customer. you need to
38:39 really understand like what is it that
38:41 is producing this data that I'm looking
38:43 at from every angle because it's very
38:46 easy to uh again lose sight of like
38:49 what's actually happening when you're
38:50 just focusing on numbers and data. All
38:53 of this makes way more sense and all of
38:55 the directions you go with your analysis
38:57 are much easier to do if you focus first
39:00 on just completely understanding the the
39:03 real world process that data represents.
39:06 So [clears throat] um a quote that I've
39:09 started saying more recently is you need
39:11 to recognize that data is a shadow of
39:13 reality not reality itself because data
39:16 is just an artifact being produced by
39:18 something and so inherently it is always
39:21 going to be an imperfect representation
39:23 of something else. And if you have
39:25 access to understanding that real
39:28 representation or whatever it is you
39:30 should focus on that because the data
39:31 will never tell you everything. it will
39:34 only be a shadow of what you're actually
39:36 trying to understand. And so if you can
39:38 understand, you know, customer buying
39:40 patterns because you yourself are a
39:41 customer, if it's, you know,
39:43 understanding the output of a machine in
39:45 a machine shop, you should be able to,
39:47 you know, deeply understand what the
39:49 machine is, all of its parts, why it's
39:51 doing what it's doing. If you can do
39:53 that, all of the data will start to make
39:54 sense. And you know, one example of uh
39:58 where where I see people trip up with
39:59 this is that people will find nuances or
40:03 patterns or significant things in data
40:05 that are actually just not real and
40:09 they're just anomalies with how the data
40:11 is being produced. And you know, people
40:13 will think, oh, I just found this new,
40:15 you know, hidden reason for why a
40:17 customer trends, when in reality, it's
40:19 just sort of like uh because the process
40:21 for logging the data is biased in some
40:22 way. So nine times out of 10 if you
40:25 observe some anomaly in data it's
40:27 probably not a real world anomaly is
40:30 probably just something to do with
40:31 either the way the data was stored or
40:33 your perspective of the data.
40:36 So you need to have this domain
40:38 knowledge like let's say if you work at
40:39 Wfware or like some other store that
40:42 sell sells furniture you need to
40:44 understand the domain like all this
40:48 let's say customer journey from like the
40:50 moment they sign up to the moment they
40:52 receive furniture happily like hopefully
40:57 like in time right so like you need to
40:59 understand all this journey and like all
41:01 the problems that can happen on the
41:02 journey from like the the first step
41:04 till the last
41:07 And the best way of getting this domain
41:10 knowledge is being a customer yourself,
41:13 right? Like if you work at a company
41:16 that sells furniture, go ahead and use
41:18 that website to order some furniture for
41:21 yourself.
41:23 Yes, absolutely. In fact, like I've
41:25 always once I realized this, I started
41:28 looking for jobs only where I was a
41:32 customer. uh because I realized how
41:34 important it was understand uh to put
41:37 myself in the shoes of the people I was
41:39 building stuff for that um so I really
41:41 wanted to be able to identify with the
41:43 customers. You know my first job my well
41:45 my first job was at Trump club but that
41:47 was only a year in data science machine
41:48 learning when I was at go health most of
41:50 our customers were uh Medicare
41:53 recipients which is people who are 65
41:54 and older in the US so I couldn't really
41:57 identify with them so I always found it
41:58 that part difficult but when I went to
42:00 Wayfair it was very easy because I was
42:02 already a wfair customer I already
42:03 purchased furniture we had just bought a
42:05 house so like I was buying furniture all
42:07 the time and so it was just like very
42:08 relevant and then same thing
42:10 the house where you live now
42:12 yeah this is the Uh [snorts]
42:15 from Wayfair.
42:16 Uh actually no. In fact, my office is
42:19 actually most of it is not Wayfair, but
42:21 a lot of the rest of the house is is
42:23 Wave.
42:25 Yeah. And uh and then the same thing
42:28 with FI, my current company. So um we
42:30 make smart dog collars and I I think my
42:34 dog is probably somewhere in my office,
42:35 but um
42:36 smart dog what?
42:37 A dog collar. So, uh, like, uh, like a
42:42 collar that they wear around their neck
42:43 and
42:44 leash of sorts.
42:46 Yeah. So, it's it's more of like a like
42:49 a a Fitbit or a GPS tracker. So, it's if
42:52 your dog escapes, you can track them,
42:54 but we also do stuff like, you know,
42:56 step counts and movement and behavior
42:58 tracking things
43:00 trackers, but for dogs.
43:02 Exactly. And so, you know, you have an
43:04 app where you can see, okay, got this
43:06 many steps today or, you know, I took my
43:08 dog for 10 miles of walks the past week
43:11 that's five miles less than what it was
43:13 a week before. So, for people who are
43:16 dog owners, it's like and interested in
43:18 like fitness or data tracking. It's like
43:20 it's a fun thing. And, you know, dogs
43:22 and data tracking and health tracking
43:24 are all things that I'm interested in.
43:26 So, to me, it was like, oh, this is a
43:28 product that I am very interested in. It
43:31 will be very natural for me to put
43:32 myself in the shoes of a customer
43:34 because I I am one essentially.
43:37 Okay. And like if you had a cat, had you
43:40 had a cat, not a dog, then for you it
43:42 would be more difficult to relate,
43:43 right?
43:44 It would be actually
43:45 you would have to like get a dog.
43:48 I would. And you know what's funny is a
43:49 lot of people who come to fight do end
43:50 up getting a dog. [laughter]
43:53 Well, that's not bad, right?
43:55 No. No, it's great.
43:58 Okay. like um so the second rule is
44:02 understand how the data is generated and
44:04 the quote you gave was realize that the
44:08 data is actually only the shadow of
44:11 reality. So you need to remember that
44:13 and like uh and make sure you understand
44:16 the process and then the data will make
44:18 more sense and you will not you will be
44:22 able to understand that the patterns you
44:23 find they're not actually like real
44:25 patterns but more like anomalies and if
44:27 you don't have this domain knowledge you
44:29 will not be able to understand that.
44:32 Exactly. Yes.
44:34 Okay. Well I guess we have time for one
44:37 or two more rules like do you have I
44:39 guess you have a bunch of them right? So
44:41 what's the third one?
44:43 Sure. So another uh unwritten rule that
44:47 I've written about many times is that
44:51 there is uh
44:53 there is a lot of uh emphasis when
44:56 teaching machine learning on mathematics
44:58 and algorithms and things like that
45:00 which are absolutely essential. However,
45:02 what one thing that I think is often
45:04 underaddressed is that to be successful
45:07 in machine learning or to be high
45:09 impact, you really need software
45:10 development skills. At the end of the
45:12 day, machine learning is software. It is
45:15 a specific type of software. But when
45:17 you build machine learning, you are
45:19 building software and it is likely going
45:21 to go into a production state. And so
45:23 being familiar, not just familiar but
45:25 like actually um you know intermediate
45:28 to advanced in software development
45:30 skills is critical to being successful
45:33 in machine learning. You know this is
45:35 maybe this is more of a I would say
45:38 sometimes a hot topic that I'll have
45:39 where not everyone agrees with me. Um
45:42 but what I can tell you is that in you
45:44 know the six years that I've been in
45:46 machine learning almost everyone it
45:49 who's been successful in machine
45:51 learning has been successful um largely
45:55 due to their skills in software
45:56 development or that they will spend the
45:59 time to get good at them because it's
46:01 very difficult to have success when you
46:04 are only able to address one part of the
46:07 machine learning funnel and you have to
46:09 hand off everything else because of
46:11 requires that everyone that you be in a
46:13 position where there are others who are
46:15 extremely competent and can work with
46:17 you to take you know hand off a model in
46:19 a Jupyter notebook and hand it off and
46:21 then deploy it. There's a lot of steps
46:22 involved there and to be reliant on
46:25 others to do that well is very limiting
46:27 in terms of your own success and it also
46:29 means that it's difficult to put you on
46:32 projects where there are there are
46:34 limited resources. you know, I can't
46:36 give somebody uh a project if I know
46:39 that like, you know, a full stack ML
46:41 project if I know that I don't have
46:43 enough MLOps resources, which is
46:45 especially true in smaller companies.
46:48 So, you need to be some sort of full
46:50 stack data science machine learning
46:51 person ideally, right? And then like if
46:54 you can do that then you can take a
46:57 project end to end like you don't need
46:59 to be an expert in all the areas but
47:00 just to know enough like to know to take
47:05 the notebook and put it as a web
47:07 service.
47:09 Yeah. And I want I want to make a
47:11 careful distinction that because a lot
47:13 of times what people think that I'm
47:14 saying is that you don't need to know
47:16 machine learning that well which isn't
47:17 true. You do need to know machine
47:18 learning. You also need to know software
47:21 engineering or software development. So
47:22 I would say it's more so that like there
47:24 is a strong balance and typically the
47:26 emphasis is on learning the machine
47:28 learning side you know again because
47:30 it's so difficult and it's very
47:32 overwhelming that even understanding
47:33 that part is so takes up all of your
47:35 mental bandwidth that thinking about
47:37 software development too is just seems
47:39 unnecessary when in reality it is very
47:42 important to understand you know can you
47:44 take a piece of code and put it into a
47:46 web server on AWS you know if you can do
47:49 that and you can do it comfortably
47:51 you're in a really spot to be able to
47:53 deploy machine learning models and be a
47:55 full stack ML ML developer.
47:58 Yeah. So what we teach in our courses,
48:00 what we say in our course is like your
48:02 model can be like super good, but if
48:04 nobody can use it, if it's just in
48:05 Jupyter notebook, then it's not good.
48:08 Like it's of no use. So you need to be
48:10 able to do that. And all these unwritten
48:13 rules were already written as you said
48:16 somewhere probably your LinkedIn uh uh
48:19 you probably published them on LinkedIn.
48:21 So, can you tell us more about like
48:23 where can people find
48:26 this content that you like all this u
48:30 all these things we talked about?
48:32 Yeah. So, uh as of right now it's all on
48:35 LinkedIn and uh I wrote I write one post
48:39 every day. Um
48:40 I post it every day uh at 6:15 a.m.
48:43 Central time.
48:45 Um and for the ones that I think are
48:47 really good and informative people, I
48:49 leave them.
48:50 Yeah. Well, I write it the day before
48:52 and then I schedule it to be uh posted
48:53 at 6:15, but then I'm up at I'm up I'm
48:57 actually up at 6 to respond to people's
48:59 comments.
49:00 Um so, uh but I leave them in my
49:04 featured section on LinkedIn at some
49:06 point. Um maybe I'll consider doing a
49:08 newsletter. I know that's commonly what
49:09 people do at the moment though. You
49:11 know, I'm still I only started writing
49:13 on LinkedIn uh a few months ago. I
49:16 started in like late July. It was the
49:17 first time I ever posted on LinkedIn. So
49:19 I'm still like, you know, even just now
49:21 I'm I'm sort of figuring out what what I
49:24 want to say and how I want to say it.
49:26 Um, so at the moment that's where it is,
49:28 but if you follow me there, you know,
49:30 you'll stay in touch with some other
49:31 places that I'll be putting this data.
49:34 And a good idea is like in LinkedIn, you
49:36 can click on this ring
49:39 icon, not ring like bell icon, and then
49:42 like the moment you do this, you'll get
49:44 notified about the posts.
49:47 Because the problem with LinkedIn is
49:48 like you need to rely on the algorithm.
49:50 So make to make sure that next time I
49:53 open it I see your post while in
49:55 newsletter you don't have this problem
49:57 right like you actually deliver and then
49:59 then it's up to me to to to decide like
50:01 do I have time to read it now or I'll
50:04 have time to read it later. So you're
50:05 not at
50:08 that's actually a good point and I
50:09 hadn't I actually had that is a very
50:11 good point. Yeah, I think you should
50:12 start a newsletter cuz like for me like
50:15 if I really want to make sure I do not
50:17 miss your content like I'd rather
50:19 subscribe to newsletter rather than rely
50:21 on the algorithm to constantly show it
50:24 to me.
50:24 Yeah,
50:25 cuz they might they decide that okay
50:27 let's show more ads and then I see you.
50:31 Actually, that's your point. I hadn't
50:33 thought about that. So maybe I'll not
50:34 maybe I'll consider that now.
50:36 Yeah. What happens on November 15th?
50:40 on November 15th. Um that will be my
50:42 last day at my at my company FI. So um I
50:46 am leaving to start a new business in uh
50:50 machine learning, data science and more
50:52 generally data uh recruiting. So my this
50:57 is there's an idea that I've been
50:59 thinking through the last couple years.
51:00 I've known I wanted to do this and I
51:02 think now is a good time which is I
51:04 think we are in a spot where we need a
51:07 big shift in how technology hiring is
51:09 done. I I'm very unhappy for many
51:11 reasons with the current state of hiring
51:14 both from an employer perspective and a
51:16 candidate perspective. I think there's
51:18 there is so much noise in the system
51:20 that it is very subjective who is
51:23 considered for a role. People don't feel
51:25 like they have any control over where
51:27 they get considered for a role. I don't
51:29 think people are interviewing for the
51:30 thing for all the things that actually
51:31 matter in success. You know, I think
51:34 through all the things I discussed today
51:35 in terms of what drives success, how
51:37 many of those things are actually
51:38 interviewed for well in in technology
51:40 interviews? Very few. So, I I I have
51:44 done hiring differently, you know,
51:46 throughout my career. I've approached it
51:48 in ways that I think makes sense. And
51:50 you know, you can take a look at the
51:51 teams that I've built over the last
51:53 five, six years and they they speak for
51:56 themselves in terms of how strong they
51:58 are. And so what I want to do is take
51:59 this and and give it to other employers
52:02 and other companies is hey here's a
52:04 here's a different way of thinking about
52:05 hiring where you can actually have a
52:07 much clearer picture around what success
52:09 looks like and for candidates to be able
52:11 to give them more control of you know
52:13 let's think through what are your
52:14 strengths how do we best showcase them
52:16 rather than you know putting you through
52:18 arbitrary uh quizzes and questions that
52:21 may not actually well align with your
52:23 strengths. So I'm going to start a
52:24 business that is in technology hiring
52:27 and recruiting and it's going to focus
52:30 uh in the short term on machine learning
52:32 and data science and uh maybe some other
52:35 data roles but in general I would like
52:37 to extend it to uh any technology role
52:40 and the idea is to be able to give
52:42 people more ability to articulate on the
52:45 candidate side what their strengths are
52:47 and showcase that make that available
52:48 than just a traditional resume. And on
52:51 the employer side, be able to both
52:53 understand what it is that they need to
52:55 succeed and be able to find that quickly
52:58 instead of having to do, you know, many
52:59 hours of interviewing on their own.
53:02 Yeah. So, we will all subscribe, follow
53:05 you on LinkedIn and we will see all the
53:07 updates about uh your new endeavor. So,
53:13 I don't like saying good luck cuz you
53:15 probably don't need luck. You more need
53:16 like perseverance, but luck also is
53:18 important. So, have fun. with your new
53:22 project and thanks a lot for coming
53:24 joining us today sharing all your
53:26 experience all these unwritten rules and
53:27 thanks everyone too. I unfortunately
53:30 have to run but it was great pleasure
53:32 talking with you and yeah have a great
53:35 rest of your week.
53:36 Likewise. Thanks like.